{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "outer-museum",
   "metadata": {},
   "source": [
    "## Single-Task GP AE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "turkish-equality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: you have not provided data variances,\n",
      "they will set to be 1 percent of the the data values!\n",
      "GP successfully initiated\n",
      "Costs successfully initialized\n",
      "##################################################################################\n",
      "Initialization successfully concluded\n",
      "now train(...) or train_async(...), and then go(...)\n",
      "##################################################################################\n",
      "GP training started with  10  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [1. 1. 1.]  with old log likelihood:  10.332088836633892\n",
      "method:  global\n",
      "I am performing a global differential evolution algorithm to find the optimal hyperparameters.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "differential_evolution step 1: f(x)= 10.4075\n",
      "differential_evolution step 2: f(x)= 10.4075\n",
      "differential_evolution step 3: f(x)= 9.76067\n",
      "differential_evolution step 4: f(x)= 9.27038\n",
      "differential_evolution step 5: f(x)= 9.27038\n",
      "differential_evolution step 6: f(x)= 9.27038\n",
      "differential_evolution step 7: f(x)= 8.12746\n",
      "differential_evolution step 8: f(x)= 8.12746\n",
      "differential_evolution step 9: f(x)= 8.12746\n",
      "differential_evolution step 10: f(x)= 8.12746\n",
      "differential_evolution step 11: f(x)= 8.12746\n",
      "differential_evolution step 12: f(x)= 8.12746\n",
      "differential_evolution step 13: f(x)= 8.12746\n",
      "differential_evolution step 14: f(x)= 8.12746\n",
      "differential_evolution step 15: f(x)= 8.12746\n",
      "differential_evolution step 16: f(x)= 8.12746\n",
      "differential_evolution step 17: f(x)= 8.12746\n",
      "differential_evolution step 18: f(x)= 8.12746\n",
      "differential_evolution step 19: f(x)= 8.06819\n",
      "differential_evolution step 20: f(x)= 7.96143\n",
      "I found hyperparameters  [0.35840661 0.39678946 8.52722459]  with likelihood  7.511592063483555  via global optimization\n",
      "Date and time:        2021-05-20_16_32_06\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  10\n",
      "Run Time:  7.748603820800781e-05      seconds\n",
      "Number of measurements:  10\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  1e-06\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.59867\n",
      "differential_evolution step 2: f(x)= -0.59867\n",
      "differential_evolution step 3: f(x)= -0.59867\n",
      "differential_evolution step 4: f(x)= -0.59867\n",
      "differential_evolution step 5: f(x)= -0.59867\n",
      "variance optimization tolerance of changed to:  0.05986701871226281\n",
      "Next points to be requested: \n",
      "[[6.17815295 1.16569435]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  11\n",
      "Run Time:  0.08795523643493652      seconds\n",
      "Number of measurements:  11\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05986701871226281\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.059815518404338956\n",
      "Next points to be requested: \n",
      "[[7.50729726 9.07007376]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  12\n",
      "Run Time:  0.09530806541442871      seconds\n",
      "Number of measurements:  12\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.059815518404338956\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.598028\n",
      "differential_evolution step 2: f(x)= -0.598163\n",
      "differential_evolution step 3: f(x)= -0.598163\n",
      "variance optimization tolerance of changed to:  0.05981632302850137\n",
      "Next points to be requested: \n",
      "[[4.83235407 8.26069581]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  13\n",
      "Run Time:  0.1284952163696289      seconds\n",
      "Number of measurements:  13\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05981632302850137\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05828173579444279\n",
      "Next points to be requested: \n",
      "[[4.03076763 3.50887239]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  14\n",
      "Run Time:  0.13310456275939941      seconds\n",
      "Number of measurements:  14\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05828173579444279\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.593065\n",
      "differential_evolution step 2: f(x)= -0.593065\n",
      "differential_evolution step 3: f(x)= -0.593065\n",
      "variance optimization tolerance of changed to:  0.059306478426154746\n",
      "Next points to be requested: \n",
      "[[9.95221366 7.34768895]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  15\n",
      "Run Time:  0.16595935821533203      seconds\n",
      "Number of measurements:  15\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.059306478426154746\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05779326710370704\n",
      "Next points to be requested: \n",
      "[[6.95820585 4.90074994]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  16\n",
      "Run Time:  0.17078280448913574      seconds\n",
      "Number of measurements:  16\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05779326710370704\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.587949\n",
      "differential_evolution step 2: f(x)= -0.587949\n",
      "differential_evolution step 3: f(x)= -0.587949\n",
      "variance optimization tolerance of changed to:  0.05879492113428669\n",
      "Next points to be requested: \n",
      "[[8.16180395 0.31571531]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  17\n",
      "Run Time:  0.20227694511413574      seconds\n",
      "Number of measurements:  17\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05879492113428669\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0562081421684179\n",
      "Next points to be requested: \n",
      "[[5.83644621 6.82972569]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  18\n",
      "Run Time:  0.20792126655578613      seconds\n",
      "Number of measurements:  18\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0562081421684179\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.557306\n",
      "differential_evolution step 2: f(x)= -0.568747\n",
      "variance optimization tolerance of changed to:  0.05687465198832947\n",
      "Next points to be requested: \n",
      "[[0.12219154 9.59710996]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  19\n",
      "Run Time:  0.23456645011901855      seconds\n",
      "Number of measurements:  19\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05687465198832947\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05355375447775046\n",
      "Next points to be requested: \n",
      "[[4.55794178 2.88693725]]\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  20  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.35840661 0.39678946 8.52722459]  with old log likelihood:  17.934751833953726\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization not successful.\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  20\n",
      "Run Time:  0.25675010681152344      seconds\n",
      "Number of measurements:  20\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05355375447775046\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.558322\n",
      "differential_evolution step 2: f(x)= -0.562473\n",
      "differential_evolution step 3: f(x)= -0.562473\n",
      "differential_evolution step 4: f(x)= -0.562728\n",
      "variance optimization tolerance of changed to:  0.056272848135895163\n",
      "Next points to be requested: \n",
      "[[5.47385654 0.42028419]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  21\n",
      "Run Time:  0.296062707901001      seconds\n",
      "Number of measurements:  21\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.056272848135895163\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05299231831359407\n",
      "Next points to be requested: \n",
      "[[3.53004142 0.18142518]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  22\n",
      "Run Time:  0.3016388416290283      seconds\n",
      "Number of measurements:  22\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05299231831359407\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.556957\n",
      "differential_evolution step 2: f(x)= -0.556957\n",
      "differential_evolution step 3: f(x)= -0.556957\n",
      "differential_evolution step 4: f(x)= -0.559903\n",
      "variance optimization tolerance of changed to:  0.05599032606144353\n",
      "Next points to be requested: \n",
      "[[2.22671729 0.1304896 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  23\n",
      "Run Time:  0.34168553352355957      seconds\n",
      "Number of measurements:  23\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05599032606144353\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05246946054219884\n",
      "Next points to be requested: \n",
      "[[1.19900092 6.95497969]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  24\n",
      "Run Time:  0.3477597236633301      seconds\n",
      "Number of measurements:  24\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05246946054219884\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.554385\n",
      "differential_evolution step 2: f(x)= -0.554385\n",
      "differential_evolution step 3: f(x)= -0.554385\n",
      "variance optimization tolerance of changed to:  0.05543846457488821\n",
      "Next points to be requested: \n",
      "[[6.55178052 9.92038857]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  25\n",
      "Run Time:  0.394045352935791      seconds\n",
      "Number of measurements:  25\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05543846457488821\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.052884938638738645\n",
      "Next points to be requested: \n",
      "[[0.         3.17185954]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  26\n",
      "Run Time:  0.4086747169494629      seconds\n",
      "Number of measurements:  26\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.052884938638738645\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.545961\n",
      "differential_evolution step 2: f(x)= -0.545961\n",
      "differential_evolution step 3: f(x)= -0.545961\n",
      "differential_evolution step 4: f(x)= -0.545976\n",
      "variance optimization tolerance of changed to:  0.05459756129798469\n",
      "Next points to be requested: \n",
      "[[7.40881475 0.18146684]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  27\n",
      "Run Time:  0.4708974361419678      seconds\n",
      "Number of measurements:  27\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05459756129798469\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05179750295303431\n",
      "Next points to be requested: \n",
      "[[2.98732612 9.23496171]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  28\n",
      "Run Time:  0.4764416217803955      seconds\n",
      "Number of measurements:  28\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05179750295303431\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.543545\n",
      "differential_evolution step 2: f(x)= -0.544331\n",
      "differential_evolution step 3: f(x)= -0.544331\n",
      "variance optimization tolerance of changed to:  0.0544330521768808\n",
      "Next points to be requested: \n",
      "[[9.55873405 0.5675578 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  29\n",
      "Run Time:  0.5090692043304443      seconds\n",
      "Number of measurements:  29\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0544330521768808\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05164509424484536\n",
      "Next points to be requested: \n",
      "[[1.20914712 0.36103197]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  30\n",
      "Run Time:  0.5141823291778564      seconds\n",
      "Number of measurements:  30\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05164509424484536\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.551845\n",
      "differential_evolution step 2: f(x)= -0.551845\n",
      "differential_evolution step 3: f(x)= -0.551845\n",
      "differential_evolution step 4: f(x)= -0.551845\n",
      "differential_evolution step 5: f(x)= -0.551845\n",
      "differential_evolution step 6: f(x)= -0.554151\n",
      "variance optimization tolerance of changed to:  0.055415090125822165\n",
      "Next points to be requested: \n",
      "[[8.30709034 9.1244469 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  31\n",
      "Run Time:  0.5704710483551025      seconds\n",
      "Number of measurements:  31\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.055415090125822165\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05107157204228492\n",
      "Next points to be requested: \n",
      "[[5.15258892 3.81751921]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  32\n",
      "Run Time:  0.5780656337738037      seconds\n",
      "Number of measurements:  32\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05107157204228492\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.540055\n",
      "differential_evolution step 2: f(x)= -0.540055\n",
      "differential_evolution step 3: f(x)= -0.540055\n",
      "differential_evolution step 4: f(x)= -0.540055\n",
      "differential_evolution step 5: f(x)= -0.543482\n",
      "differential_evolution step 6: f(x)= -0.551775\n",
      "differential_evolution step 7: f(x)= -0.551775\n",
      "variance optimization tolerance of changed to:  0.055177526185632764\n",
      "Next points to be requested: \n",
      "[[2.18206185 9.94890495]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  33\n",
      "Run Time:  0.6437349319458008      seconds\n",
      "Number of measurements:  33\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.055177526185632764\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance optimization tolerance of changed to:  0.043778326274840455\n",
      "Next points to be requested: \n",
      "[[9.19593829 0.83260238]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  34\n",
      "Run Time:  0.6487240791320801      seconds\n",
      "Number of measurements:  34\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.043778326274840455\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.529859\n",
      "differential_evolution step 2: f(x)= -0.529859\n",
      "differential_evolution step 3: f(x)= -0.529859\n",
      "differential_evolution step 4: f(x)= -0.529859\n",
      "differential_evolution step 5: f(x)= -0.529859\n",
      "differential_evolution step 6: f(x)= -0.529859\n",
      "differential_evolution step 7: f(x)= -0.529859\n",
      "variance optimization tolerance of changed to:  0.052985887577494575\n",
      "Next points to be requested: \n",
      "[[8.63016    0.31982928]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  35\n",
      "Run Time:  0.7145438194274902      seconds\n",
      "Number of measurements:  35\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.052985887577494575\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.048085697105318084\n",
      "Next points to be requested: \n",
      "[[1.82855224 8.63602312]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  36\n",
      "Run Time:  0.7211461067199707      seconds\n",
      "Number of measurements:  36\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.048085697105318084\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.509305\n",
      "differential_evolution step 2: f(x)= -0.515147\n",
      "differential_evolution step 3: f(x)= -0.51734\n",
      "differential_evolution step 4: f(x)= -0.5201\n",
      "differential_evolution step 5: f(x)= -0.526068\n",
      "differential_evolution step 6: f(x)= -0.526068\n",
      "differential_evolution step 7: f(x)= -0.526068\n",
      "differential_evolution step 8: f(x)= -0.526068\n",
      "differential_evolution step 9: f(x)= -0.526068\n",
      "variance optimization tolerance of changed to:  0.05260678363635834\n",
      "Next points to be requested: \n",
      "[[4.35195467 9.73337725]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  37\n",
      "Run Time:  0.8035569190979004      seconds\n",
      "Number of measurements:  37\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05260678363635834\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04314789318301195\n",
      "Next points to be requested: \n",
      "[[2.62438236 7.69749359]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  38\n",
      "Run Time:  0.8092668056488037      seconds\n",
      "Number of measurements:  38\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04314789318301195\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.522838\n",
      "differential_evolution step 2: f(x)= -0.522838\n",
      "differential_evolution step 3: f(x)= -0.522838\n",
      "differential_evolution step 4: f(x)= -0.522838\n",
      "differential_evolution step 5: f(x)= -0.522838\n",
      "differential_evolution step 6: f(x)= -0.522838\n",
      "differential_evolution step 7: f(x)= -0.522838\n",
      "differential_evolution step 8: f(x)= -0.522838\n",
      "differential_evolution step 9: f(x)= -0.522838\n",
      "differential_evolution step 10: f(x)= -0.522838\n",
      "differential_evolution step 11: f(x)= -0.524603\n",
      "differential_evolution step 12: f(x)= -0.524603\n",
      "variance optimization tolerance of changed to:  0.05246029581275328\n",
      "Next points to be requested: \n",
      "[[9.51878104 9.55072787]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "GPOptimizer updated the Hyperperameters:  [0.35840661 0.39678946 8.52722459]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  39\n",
      "Run Time:  0.959514856338501      seconds\n",
      "Number of measurements:  39\n",
      "====================\n",
      "hps:  [0.35840661 0.39678946 8.52722459]\n",
      "aks() initiated with hyperparameters: [0.35840661 0.39678946 8.52722459]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05246029581275328\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04383134132268591\n",
      "Next points to be requested: \n",
      "[[3.18489065 4.78089154]]\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  40  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.35840661 0.39678946 8.52722459]  with old log likelihood:  49.07310259083082\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.35840661 0.39678946 8.52722459]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization successfully concluded with result:  38.33460291518716\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  40\n",
      "Run Time:  0.9838383197784424      seconds\n",
      "Number of measurements:  40\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04383134132268591\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793561008815\n",
      "Next points to be requested: \n",
      "[[7.77471065 5.54727916]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  41\n",
      "Run Time:  1.0054931640625      seconds\n",
      "Number of measurements:  41\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793561008815\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295677184750133\n",
      "Next points to be requested: \n",
      "[[8.70466294 9.6635908 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  42\n",
      "Run Time:  1.0119903087615967      seconds\n",
      "Number of measurements:  42\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295677184750133\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793561005371\n",
      "Next points to be requested: \n",
      "[[7.16435822 3.41842311]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  43\n",
      "Run Time:  1.0303220748901367      seconds\n",
      "Number of measurements:  43\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793561005371\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295783974225795\n",
      "Next points to be requested: \n",
      "[[6.45721043 4.71163698]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  44\n",
      "Run Time:  1.0359151363372803      seconds\n",
      "Number of measurements:  44\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295783974225795\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793561008796\n",
      "Next points to be requested: \n",
      "[[3.78104137 3.35548894]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  45\n",
      "Run Time:  1.0555036067962646      seconds\n",
      "Number of measurements:  45\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793561008796\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0629579224921679\n",
      "Next points to be requested: \n",
      "[[3.30262515 0.37791708]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  46\n",
      "Run Time:  1.0610332489013672      seconds\n",
      "Number of measurements:  46\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0629579224921679\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793560017961\n",
      "Next points to be requested: \n",
      "[[0.33650473 2.93823088]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  47\n",
      "Run Time:  1.0808453559875488      seconds\n",
      "Number of measurements:  47\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793560017961\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295793510154057\n",
      "Next points to be requested: \n",
      "[[2.49082902 0.27740749]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  48\n",
      "Run Time:  1.0858771800994873      seconds\n",
      "Number of measurements:  48\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295793510154057\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.0629579356099462\n",
      "Next points to be requested: \n",
      "[[6.76154731 3.33875154]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  49\n",
      "Run Time:  1.1072323322296143      seconds\n",
      "Number of measurements:  49\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0629579356099462\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295792933937\n",
      "Next points to be requested: \n",
      "[[5.03732088 8.2527864 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  50\n",
      "Run Time:  1.1123616695404053      seconds\n",
      "Number of measurements:  50\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295792933937\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793560763789\n",
      "Next points to be requested: \n",
      "[[9.77802504 3.60573095]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  51\n",
      "Run Time:  1.1310811042785645      seconds\n",
      "Number of measurements:  51\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793560763789\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295777303741365\n",
      "Next points to be requested: \n",
      "[[7.86580568 3.70381213]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  52\n",
      "Run Time:  1.1362299919128418      seconds\n",
      "Number of measurements:  52\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295777303741365\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793560920747\n",
      "Next points to be requested: \n",
      "[[1.40361649 7.97914995]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  53\n",
      "Run Time:  1.1551017761230469      seconds\n",
      "Number of measurements:  53\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793560920747\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295261103689898\n",
      "Next points to be requested: \n",
      "[[8.09694343 0.88687687]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  54\n",
      "Run Time:  1.1602990627288818      seconds\n",
      "Number of measurements:  54\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295261103689898\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793559586338\n",
      "Next points to be requested: \n",
      "[[5.67547702 2.98370158]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  55\n",
      "Run Time:  1.1828649044036865      seconds\n",
      "Number of measurements:  55\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793559586338\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06295793100378203\n",
      "Next points to be requested: \n",
      "[[5.95416512 4.55294866]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  56\n",
      "Run Time:  1.1902239322662354      seconds\n",
      "Number of measurements:  56\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06295793100378203\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793560896833\n",
      "Next points to be requested: \n",
      "[[1.01854038 8.95670016]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  57\n",
      "Run Time:  1.211303949356079      seconds\n",
      "Number of measurements:  57\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793560896833\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0629147552543404\n",
      "Next points to be requested: \n",
      "[[10.        0.390138]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  58\n",
      "Run Time:  1.2174487113952637      seconds\n",
      "Number of measurements:  58\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0629147552543404\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.629579\n",
      "variance optimization tolerance of changed to:  0.06295793555491523\n",
      "Next points to be requested: \n",
      "[[8.45831715 4.7498    ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "GPOptimizer updated the Hyperperameters:  [0.39637017 0.01497928 8.49702113]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  59\n",
      "Run Time:  1.2474117279052734      seconds\n",
      "Number of measurements:  59\n",
      "====================\n",
      "hps:  [0.39637017 0.01497928 8.49702113]\n",
      "aks() initiated with hyperparameters: [0.39637017 0.01497928 8.49702113]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06295793555491523\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06293071085332318\n",
      "Next points to be requested: \n",
      "[[6.70989388 9.11633295]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  60  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.39637017 0.01497928 8.49702113]  with old log likelihood:  59.94673911274633\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.39637017 0.01497928 8.49702113]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization successfully concluded with result:  59.86462547635053\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  60\n",
      "Run Time:  1.2763018608093262      seconds\n",
      "Number of measurements:  60\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06293071085332318\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053838554411\n",
      "Next points to be requested: \n",
      "[[0.66207557 0.90248404]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  61\n",
      "Run Time:  1.3079850673675537      seconds\n",
      "Number of measurements:  61\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053838554411\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05685474035450763\n",
      "Next points to be requested: \n",
      "[[0.        9.1815693]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  62\n",
      "Run Time:  1.3261463642120361      seconds\n",
      "Number of measurements:  62\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05685474035450763\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053839693849\n",
      "Next points to be requested: \n",
      "[[9.34877233 4.29713018]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  63\n",
      "Run Time:  1.3693866729736328      seconds\n",
      "Number of measurements:  63\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053839693849\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06534014413223135\n",
      "Next points to be requested: \n",
      "[[4.26836267 9.0476615 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  64\n",
      "Run Time:  1.3797953128814697      seconds\n",
      "Number of measurements:  64\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06534014413223135\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053740535875\n",
      "Next points to be requested: \n",
      "[[2.05459678 6.07535199]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  65\n",
      "Run Time:  1.4071509838104248      seconds\n",
      "Number of measurements:  65\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053740535875\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06531746876281791\n",
      "Next points to be requested: \n",
      "[[5.89347844 6.839481  ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  66\n",
      "Run Time:  1.4125826358795166      seconds\n",
      "Number of measurements:  66\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06531746876281791\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053804647694\n",
      "Next points to be requested: \n",
      "[[2.81424524 4.50074522]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  67\n",
      "Run Time:  1.4344751834869385      seconds\n",
      "Number of measurements:  67\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053804647694\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06465606455561788\n",
      "Next points to be requested: \n",
      "[[2.46235533 1.2859876 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  68\n",
      "Run Time:  1.4438989162445068      seconds\n",
      "Number of measurements:  68\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06465606455561788\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053840081831\n",
      "Next points to be requested: \n",
      "[[5.30683008 5.41866939]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  69\n",
      "Run Time:  1.4661228656768799      seconds\n",
      "Number of measurements:  69\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053840081831\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06534050614118252\n",
      "Next points to be requested: \n",
      "[[1.95246004 0.44079781]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  70\n",
      "Run Time:  1.47434663772583      seconds\n",
      "Number of measurements:  70\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06534050614118252\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053563290504\n",
      "Next points to be requested: \n",
      "[[3.65721694 1.04405147]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  71\n",
      "Run Time:  1.4978649616241455      seconds\n",
      "Number of measurements:  71\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053563290504\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06527503663068158\n",
      "Next points to be requested: \n",
      "[[2.00669253 0.83736751]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  72\n",
      "Run Time:  1.5033128261566162      seconds\n",
      "Number of measurements:  72\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06527503663068158\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053788529304\n",
      "Next points to be requested: \n",
      "[[4.69671049 0.51054418]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  73\n",
      "Run Time:  1.5244734287261963      seconds\n",
      "Number of measurements:  73\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053788529304\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06534051381164906\n",
      "Next points to be requested: \n",
      "[[4.13480869 7.54084368]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  74\n",
      "Run Time:  1.5320093631744385      seconds\n",
      "Number of measurements:  74\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06534051381164906\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053633174638\n",
      "Next points to be requested: \n",
      "[[6.30056164 4.49556584]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  75\n",
      "Run Time:  1.5537407398223877      seconds\n",
      "Number of measurements:  75\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053633174638\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06528850583816596\n",
      "Next points to be requested: \n",
      "[[1.4510855  8.72326527]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  76\n",
      "Run Time:  1.5637273788452148      seconds\n",
      "Number of measurements:  76\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06528850583816596\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053593654164\n",
      "Next points to be requested: \n",
      "[[9.05522816 9.65864918]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  77\n",
      "Run Time:  1.59116792678833      seconds\n",
      "Number of measurements:  77\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053593654164\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06534022087919496\n",
      "Next points to be requested: \n",
      "[[4.91773535 9.54430374]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  78\n",
      "Run Time:  1.5978343486785889      seconds\n",
      "Number of measurements:  78\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06534022087919496\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.653405\n",
      "variance optimization tolerance of changed to:  0.06534053148625521\n",
      "Next points to be requested: \n",
      "[[7.66143313 0.3402491 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "GPOptimizer updated the Hyperperameters:  [0.4269386  0.01479266 8.49727735]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  79\n",
      "Run Time:  1.6205337047576904      seconds\n",
      "Number of measurements:  79\n",
      "====================\n",
      "hps:  [0.4269386  0.01479266 8.49727735]\n",
      "aks() initiated with hyperparameters: [0.4269386  0.01479266 8.49727735]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.06534053148625521\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.06533988415828285\n",
      "Next points to be requested: \n",
      "[[9.43507343 7.48996144]]\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  80  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.4269386  0.01479266 8.49727735]  with old log likelihood:  81.86919936588971\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.4269386  0.01479266 8.49727735]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local optimization successfully concluded with result:  29.430660200616764\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  80\n",
      "Run Time:  1.6735649108886719      seconds\n",
      "Number of measurements:  80\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.06533988415828285\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.375083\n",
      "differential_evolution step 2: f(x)= -0.388689\n",
      "differential_evolution step 3: f(x)= -0.391198\n",
      "differential_evolution step 4: f(x)= -0.403843\n",
      "differential_evolution step 5: f(x)= -0.414618\n",
      "differential_evolution step 6: f(x)= -0.414618\n",
      "differential_evolution step 7: f(x)= -0.432518\n",
      "differential_evolution step 8: f(x)= -0.432518\n",
      "differential_evolution step 9: f(x)= -0.432518\n",
      "differential_evolution step 10: f(x)= -0.432518\n",
      "differential_evolution step 11: f(x)= -0.437014\n",
      "differential_evolution step 12: f(x)= -0.438302\n",
      "differential_evolution step 13: f(x)= -0.439033\n",
      "variance optimization tolerance of changed to:  0.04390332872090602\n",
      "Next points to be requested: \n",
      "[[0.01113917 5.52336381]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  81\n",
      "Run Time:  1.8506536483764648      seconds\n",
      "Number of measurements:  81\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04390332872090602\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03413023841980443\n",
      "Next points to be requested: \n",
      "[[1.37298565 1.87184919]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  82\n",
      "Run Time:  1.8621971607208252      seconds\n",
      "Number of measurements:  82\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03413023841980443\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.395585\n",
      "differential_evolution step 2: f(x)= -0.395585\n",
      "differential_evolution step 3: f(x)= -0.395585\n",
      "differential_evolution step 4: f(x)= -0.398792\n",
      "differential_evolution step 5: f(x)= -0.398792\n",
      "differential_evolution step 6: f(x)= -0.398792\n",
      "differential_evolution step 7: f(x)= -0.404394\n",
      "differential_evolution step 8: f(x)= -0.404394\n",
      "differential_evolution step 9: f(x)= -0.404394\n",
      "differential_evolution step 10: f(x)= -0.408916\n",
      "differential_evolution step 11: f(x)= -0.409219\n",
      "differential_evolution step 12: f(x)= -0.411047\n",
      "differential_evolution step 13: f(x)= -0.413368\n",
      "variance optimization tolerance of changed to:  0.0413367632003663\n",
      "Next points to be requested: \n",
      "[[9.99488165 2.1839451 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  83\n",
      "Run Time:  2.0030410289764404      seconds\n",
      "Number of measurements:  83\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0413367632003663\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03306180156086725\n",
      "Next points to be requested: \n",
      "[[7.98500285 2.93137601]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  84\n",
      "Run Time:  2.00981068611145      seconds\n",
      "Number of measurements:  84\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03306180156086725\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.37764\n",
      "differential_evolution step 2: f(x)= -0.37764\n",
      "differential_evolution step 3: f(x)= -0.3801\n",
      "differential_evolution step 4: f(x)= -0.3801\n",
      "differential_evolution step 5: f(x)= -0.3801\n",
      "differential_evolution step 6: f(x)= -0.3801\n",
      "differential_evolution step 7: f(x)= -0.385922\n",
      "differential_evolution step 8: f(x)= -0.385922\n",
      "differential_evolution step 9: f(x)= -0.385922\n",
      "differential_evolution step 10: f(x)= -0.385922\n",
      "differential_evolution step 11: f(x)= -0.38873\n",
      "differential_evolution step 12: f(x)= -0.38873\n",
      "differential_evolution step 13: f(x)= -0.389009\n",
      "differential_evolution step 14: f(x)= -0.389572\n",
      "differential_evolution step 15: f(x)= -0.389751\n",
      "variance optimization tolerance of changed to:  0.038975103481950685\n",
      "Next points to be requested: \n",
      "[[6.49878641e-03 7.53893671e+00]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  85\n",
      "Run Time:  2.2081449031829834      seconds\n",
      "Number of measurements:  85\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.038975103481950685\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03286098881322696\n",
      "Next points to be requested: \n",
      "[[9.38467028 8.6852799 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  86\n",
      "Run Time:  2.217960834503174      seconds\n",
      "Number of measurements:  86\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03286098881322696\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.390676\n",
      "differential_evolution step 2: f(x)= -0.390676\n",
      "differential_evolution step 3: f(x)= -0.390676\n",
      "differential_evolution step 4: f(x)= -0.390676\n",
      "differential_evolution step 5: f(x)= -0.390676\n",
      "differential_evolution step 6: f(x)= -0.390676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 7: f(x)= -0.397725\n",
      "differential_evolution step 8: f(x)= -0.397725\n",
      "differential_evolution step 9: f(x)= -0.397725\n",
      "differential_evolution step 10: f(x)= -0.397725\n",
      "differential_evolution step 11: f(x)= -0.397725\n",
      "differential_evolution step 12: f(x)= -0.397725\n",
      "differential_evolution step 13: f(x)= -0.402831\n",
      "differential_evolution step 14: f(x)= -0.403504\n",
      "differential_evolution step 15: f(x)= -0.403504\n",
      "differential_evolution step 16: f(x)= -0.403991\n",
      "variance optimization tolerance of changed to:  0.04039906086064055\n",
      "Next points to be requested: \n",
      "[[9.98775356 5.65294668]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  87\n",
      "Run Time:  2.389808177947998      seconds\n",
      "Number of measurements:  87\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04039906086064055\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.030331867570059193\n",
      "Next points to be requested: \n",
      "[[3.94048236 4.59987824]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  88\n",
      "Run Time:  2.397576332092285      seconds\n",
      "Number of measurements:  88\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.030331867570059193\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.367994\n",
      "differential_evolution step 2: f(x)= -0.367994\n",
      "differential_evolution step 3: f(x)= -0.37027\n",
      "differential_evolution step 4: f(x)= -0.371798\n",
      "differential_evolution step 5: f(x)= -0.373885\n",
      "differential_evolution step 6: f(x)= -0.375187\n",
      "differential_evolution step 7: f(x)= -0.375187\n",
      "differential_evolution step 8: f(x)= -0.375187\n",
      "variance optimization tolerance of changed to:  0.03751873923692914\n",
      "Next points to be requested: \n",
      "[[7.39912839 7.56225828]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  89\n",
      "Run Time:  2.486093521118164      seconds\n",
      "Number of measurements:  89\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03751873923692914\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.028720764792527626\n",
      "Next points to be requested: \n",
      "[[6.22200131 0.40806012]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  90\n",
      "Run Time:  2.4927480220794678      seconds\n",
      "Number of measurements:  90\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.028720764792527626\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.356042\n",
      "differential_evolution step 2: f(x)= -0.356042\n",
      "differential_evolution step 3: f(x)= -0.356042\n",
      "differential_evolution step 4: f(x)= -0.358177\n",
      "differential_evolution step 5: f(x)= -0.358177\n",
      "differential_evolution step 6: f(x)= -0.358177\n",
      "differential_evolution step 7: f(x)= -0.358611\n",
      "differential_evolution step 8: f(x)= -0.359025\n",
      "differential_evolution step 9: f(x)= -0.360073\n",
      "differential_evolution step 10: f(x)= -0.360073\n",
      "differential_evolution step 11: f(x)= -0.360073\n",
      "variance optimization tolerance of changed to:  0.036007305336628936\n",
      "Next points to be requested: \n",
      "[[3.81507585 6.18100059]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  91\n",
      "Run Time:  2.6121742725372314      seconds\n",
      "Number of measurements:  91\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.036007305336628936\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02554510704212475\n",
      "Next points to be requested: \n",
      "[[1.48316197 1.37419584]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  92\n",
      "Run Time:  2.619168758392334      seconds\n",
      "Number of measurements:  92\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02554510704212475\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.342417\n",
      "differential_evolution step 2: f(x)= -0.343056\n",
      "differential_evolution step 3: f(x)= -0.343056\n",
      "differential_evolution step 4: f(x)= -0.343056\n",
      "differential_evolution step 5: f(x)= -0.343454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 6: f(x)= -0.343454\n",
      "differential_evolution step 7: f(x)= -0.343501\n",
      "variance optimization tolerance of changed to:  0.03435009968081409\n",
      "Next points to be requested: \n",
      "[[7.30080088 1.96691233]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  93\n",
      "Run Time:  2.7368295192718506      seconds\n",
      "Number of measurements:  93\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03435009968081409\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03285181012948416\n",
      "Next points to be requested: \n",
      "[[2.83127027 2.0352037 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  94\n",
      "Run Time:  2.7481484413146973      seconds\n",
      "Number of measurements:  94\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03285181012948416\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.325541\n",
      "differential_evolution step 2: f(x)= -0.325541\n",
      "differential_evolution step 3: f(x)= -0.325541\n",
      "differential_evolution step 4: f(x)= -0.328594\n",
      "differential_evolution step 5: f(x)= -0.328594\n",
      "differential_evolution step 6: f(x)= -0.328594\n",
      "differential_evolution step 7: f(x)= -0.328594\n",
      "differential_evolution step 8: f(x)= -0.331751\n",
      "differential_evolution step 9: f(x)= -0.331751\n",
      "differential_evolution step 10: f(x)= -0.334787\n",
      "differential_evolution step 11: f(x)= -0.334787\n",
      "differential_evolution step 12: f(x)= -0.334787\n",
      "differential_evolution step 13: f(x)= -0.336471\n",
      "differential_evolution step 14: f(x)= -0.336881\n",
      "variance optimization tolerance of changed to:  0.03368805751731189\n",
      "Next points to be requested: \n",
      "[[4.76089704 1.82085105]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  95\n",
      "Run Time:  2.905592679977417      seconds\n",
      "Number of measurements:  95\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03368805751731189\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.030522017965723125\n",
      "Next points to be requested: \n",
      "[[7.81025657 6.9814669 ]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  96\n",
      "Run Time:  2.912043571472168      seconds\n",
      "Number of measurements:  96\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.030522017965723125\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.373681\n",
      "differential_evolution step 2: f(x)= -0.373681\n",
      "differential_evolution step 3: f(x)= -0.38302\n",
      "differential_evolution step 4: f(x)= -0.38302\n",
      "differential_evolution step 5: f(x)= -0.416931\n",
      "differential_evolution step 6: f(x)= -0.416931\n",
      "differential_evolution step 7: f(x)= -0.416931\n",
      "differential_evolution step 8: f(x)= -0.416931\n",
      "differential_evolution step 9: f(x)= -0.417939\n",
      "differential_evolution step 10: f(x)= -0.417939\n",
      "differential_evolution step 11: f(x)= -0.417939\n",
      "differential_evolution step 12: f(x)= -0.417939\n",
      "differential_evolution step 13: f(x)= -0.417939\n",
      "differential_evolution step 14: f(x)= -0.417939\n",
      "differential_evolution step 15: f(x)= -0.417939\n",
      "differential_evolution step 16: f(x)= -0.420097\n",
      "differential_evolution step 17: f(x)= -0.422212\n",
      "differential_evolution step 18: f(x)= -0.422212\n",
      "differential_evolution step 19: f(x)= -0.422212\n",
      "differential_evolution step 20: f(x)= -0.424843\n",
      "variance optimization tolerance of changed to:  0.04248430544151914\n",
      "Next points to be requested: \n",
      "[[0.00374772 0.00955282]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  97\n",
      "Run Time:  3.164290428161621      seconds\n",
      "Number of measurements:  97\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04248430544151914\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.028564710094663638\n",
      "Next points to be requested: \n",
      "[[4.13048094 5.19384496]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  98\n",
      "Run Time:  3.1719932556152344      seconds\n",
      "Number of measurements:  98\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.028564710094663638\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.350763\n",
      "differential_evolution step 2: f(x)= -0.350763\n",
      "differential_evolution step 3: f(x)= -0.350763\n",
      "differential_evolution step 4: f(x)= -0.350763\n",
      "differential_evolution step 5: f(x)= -0.350763\n",
      "differential_evolution step 6: f(x)= -0.350763\n",
      "differential_evolution step 7: f(x)= -0.350763\n",
      "differential_evolution step 8: f(x)= -0.350763\n",
      "differential_evolution step 9: f(x)= -0.353343\n",
      "differential_evolution step 10: f(x)= -0.353343\n",
      "differential_evolution step 11: f(x)= -0.353343\n",
      "differential_evolution step 12: f(x)= -0.354846\n",
      "differential_evolution step 13: f(x)= -0.35627\n",
      "differential_evolution step 14: f(x)= -0.357397\n",
      "differential_evolution step 15: f(x)= -0.357397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 16: f(x)= -0.357397\n",
      "differential_evolution step 17: f(x)= -0.357398\n",
      "differential_evolution step 18: f(x)= -0.357602\n",
      "differential_evolution step 19: f(x)= -0.357956\n",
      "variance optimization tolerance of changed to:  0.035795562710801476\n",
      "Next points to be requested: \n",
      "[[6.93425789e-04 1.78827777e+00]]\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.6696687  6.72899152 5.54072255]\n",
      "GPOptimizer updated the Hyperperameters:  [0.6696687  6.72899152 5.54072255]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  99\n",
      "Run Time:  3.3819046020507812      seconds\n",
      "Number of measurements:  99\n",
      "====================\n",
      "hps:  [0.6696687  6.72899152 5.54072255]\n",
      "aks() initiated with hyperparameters: [0.6696687  6.72899152 5.54072255]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.035795562710801476\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.029764391301548728\n",
      "Next points to be requested: \n",
      "[[6.15051205 5.53385456]]\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  100  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.6696687  6.72899152 5.54072255]  with old log likelihood:  34.661567943119394\n",
      "method:  global\n",
      "I am performing a global differential evolution algorithm to find the optimal hyperparameters.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "differential_evolution step 1: f(x)= 35.8066\n",
      "differential_evolution step 2: f(x)= 35.5465\n",
      "differential_evolution step 3: f(x)= 35.5465\n",
      "differential_evolution step 4: f(x)= 35.5465\n",
      "differential_evolution step 5: f(x)= 35.5465\n",
      "differential_evolution step 6: f(x)= 35.5465\n",
      "differential_evolution step 7: f(x)= 35.4626\n",
      "differential_evolution step 8: f(x)= 35.2727\n",
      "differential_evolution step 9: f(x)= 35.1694\n",
      "differential_evolution step 10: f(x)= 35.1694\n",
      "differential_evolution step 11: f(x)= 35.1694\n",
      "differential_evolution step 12: f(x)= 34.8824\n",
      "differential_evolution step 13: f(x)= 34.8824\n",
      "differential_evolution step 14: f(x)= 34.7873\n",
      "differential_evolution step 15: f(x)= 34.4924\n",
      "differential_evolution step 16: f(x)= 34.4924\n",
      "differential_evolution step 17: f(x)= 34.3215\n",
      "differential_evolution step 18: f(x)= 34.3215\n",
      "differential_evolution step 19: f(x)= 34.2809\n",
      "differential_evolution step 20: f(x)= 34.2805\n",
      "I found hyperparameters  [0.71093544 6.76037901 6.57570835]  with likelihood  34.02214948174336  via global optimization\n",
      "====================================================\n",
      "The autonomous experiment was concluded successfully\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/env python\n",
    "import numpy as np\n",
    "from gpcam.autonomous_experimenter import AutonomousExperimenterGP\n",
    "\n",
    "def instrument(data):\n",
    "    for entry in data:\n",
    "        entry[\"value\"] = np.sin(np.linalg.norm(entry[\"position\"]))\n",
    "    return data\n",
    "\n",
    "my_ae = AutonomousExperimenterGP(np.array([[0,10],[0,10]]),instrument,\n",
    "                                 np.ones((3)),np.array([[0.001,100],[0.001,100],[0.001,100]]),\n",
    "                                 init_dataset_size= 10)\n",
    "my_ae.train()\n",
    "my_ae.go(N = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-commander",
   "metadata": {},
   "source": [
    "## Multi-Task GP AE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "answering-trademark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Costs successfully initialized\n",
      "##################################################################################\n",
      "Initialization successfully concluded\n",
      "now train(...) or train_async(...), and then go(...)\n",
      "##################################################################################\n",
      "GP training started with  20  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [1. 1. 1.]  with old log likelihood:  -4.461908310598226\n",
      "method:  global\n",
      "I am performing a global differential evolution algorithm to find the optimal hyperparameters.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "differential_evolution step 1: f(x)= -7.86196\n",
      "differential_evolution step 2: f(x)= -8.57567\n",
      "differential_evolution step 3: f(x)= -8.57567\n",
      "differential_evolution step 4: f(x)= -8.97618\n",
      "differential_evolution step 5: f(x)= -8.97618\n",
      "differential_evolution step 6: f(x)= -8.97618\n",
      "differential_evolution step 7: f(x)= -8.97618\n",
      "differential_evolution step 8: f(x)= -9.03217\n",
      "differential_evolution step 9: f(x)= -9.03217\n",
      "differential_evolution step 10: f(x)= -9.15669\n",
      "differential_evolution step 11: f(x)= -9.21562\n",
      "differential_evolution step 12: f(x)= -9.3843\n",
      "differential_evolution step 13: f(x)= -9.48692\n",
      "differential_evolution step 14: f(x)= -9.52152\n",
      "differential_evolution step 15: f(x)= -9.52152\n",
      "differential_evolution step 16: f(x)= -9.52152\n",
      "differential_evolution step 17: f(x)= -9.52152\n",
      "differential_evolution step 18: f(x)= -9.52152\n",
      "differential_evolution step 19: f(x)= -9.52433\n",
      "differential_evolution step 20: f(x)= -9.52433\n",
      "I found hyperparameters  [ 0.55454893 10.07998974 13.05908447]  with likelihood  -9.551398085799931  via global optimization\n",
      "Date and time:        2021-05-20_16_32_13\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  10\n",
      "Run Time:  0.00022220611572265625      seconds\n",
      "Number of measurements:  10\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  1e-06\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.430823\n",
      "differential_evolution step 2: f(x)= -0.433944\n",
      "differential_evolution step 3: f(x)= -0.434264\n",
      "differential_evolution step 4: f(x)= -0.451991\n",
      "differential_evolution step 5: f(x)= -0.451991\n",
      "differential_evolution step 6: f(x)= -0.474159\n",
      "differential_evolution step 7: f(x)= -0.493981\n",
      "differential_evolution step 8: f(x)= -0.493981\n",
      "differential_evolution step 9: f(x)= -0.493981\n",
      "differential_evolution step 10: f(x)= -0.493981\n",
      "differential_evolution step 11: f(x)= -0.493981\n",
      "differential_evolution step 12: f(x)= -0.493981\n",
      "differential_evolution step 13: f(x)= -0.493981\n",
      "differential_evolution step 14: f(x)= -0.494551\n",
      "differential_evolution step 15: f(x)= -0.494551\n",
      "differential_evolution step 16: f(x)= -0.494551\n",
      "differential_evolution step 17: f(x)= -0.494551\n",
      "differential_evolution step 18: f(x)= -0.495761\n",
      "differential_evolution step 19: f(x)= -0.49625\n",
      "differential_evolution step 20: f(x)= -0.49625\n",
      "variance optimization tolerance of changed to:  0.049625041140149435\n",
      "Next points to be requested: \n",
      "[[9.99301713e+00 4.62373956e-03]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  11\n",
      "Run Time:  0.20144987106323242      seconds\n",
      "Number of measurements:  11\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.049625041140149435\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03662697702064701\n",
      "Next points to be requested: \n",
      "[[3.05956448 5.30791111]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  12\n",
      "Run Time:  0.2069385051727295      seconds\n",
      "Number of measurements:  12\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03662697702064701\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.447492\n",
      "differential_evolution step 2: f(x)= -0.447492\n",
      "differential_evolution step 3: f(x)= -0.447492\n",
      "differential_evolution step 4: f(x)= -0.466586\n",
      "differential_evolution step 5: f(x)= -0.466586\n",
      "differential_evolution step 6: f(x)= -0.466586\n",
      "differential_evolution step 7: f(x)= -0.466586\n",
      "differential_evolution step 8: f(x)= -0.466586\n",
      "differential_evolution step 9: f(x)= -0.470071\n",
      "differential_evolution step 10: f(x)= -0.471173\n",
      "differential_evolution step 11: f(x)= -0.471173\n",
      "differential_evolution step 12: f(x)= -0.471173\n",
      "differential_evolution step 13: f(x)= -0.471692\n",
      "variance optimization tolerance of changed to:  0.04716920049345794\n",
      "Next points to be requested: \n",
      "[[9.98063678 9.96487854]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  13\n",
      "Run Time:  0.3240180015563965      seconds\n",
      "Number of measurements:  13\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04716920049345794\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03256524820872029\n",
      "Next points to be requested: \n",
      "[[1.9022617  3.80424917]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  14\n",
      "Run Time:  0.32854342460632324      seconds\n",
      "Number of measurements:  14\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03256524820872029\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.390002\n",
      "differential_evolution step 2: f(x)= -0.390002\n",
      "differential_evolution step 3: f(x)= -0.390002\n",
      "differential_evolution step 4: f(x)= -0.390002\n",
      "differential_evolution step 5: f(x)= -0.390002\n",
      "differential_evolution step 6: f(x)= -0.390287\n",
      "differential_evolution step 7: f(x)= -0.390287\n",
      "differential_evolution step 8: f(x)= -0.394144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 9: f(x)= -0.395242\n",
      "differential_evolution step 10: f(x)= -0.404333\n",
      "differential_evolution step 11: f(x)= -0.404333\n",
      "variance optimization tolerance of changed to:  0.040433339182629174\n",
      "Next points to be requested: \n",
      "[[0.00134336 0.02914741]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  15\n",
      "Run Time:  0.43061232566833496      seconds\n",
      "Number of measurements:  15\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.040433339182629174\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03126238268434005\n",
      "Next points to be requested: \n",
      "[[3.03554862 0.17747432]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  16\n",
      "Run Time:  0.4360668659210205      seconds\n",
      "Number of measurements:  16\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03126238268434005\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.351551\n",
      "differential_evolution step 2: f(x)= -0.386642\n",
      "differential_evolution step 3: f(x)= -0.402331\n",
      "differential_evolution step 4: f(x)= -0.408531\n",
      "differential_evolution step 5: f(x)= -0.412415\n",
      "differential_evolution step 6: f(x)= -0.412415\n",
      "differential_evolution step 7: f(x)= -0.412415\n",
      "differential_evolution step 8: f(x)= -0.422044\n",
      "differential_evolution step 9: f(x)= -0.422044\n",
      "differential_evolution step 10: f(x)= -0.424186\n",
      "differential_evolution step 11: f(x)= -0.424186\n",
      "differential_evolution step 12: f(x)= -0.424186\n",
      "differential_evolution step 13: f(x)= -0.425286\n",
      "differential_evolution step 14: f(x)= -0.427194\n",
      "differential_evolution step 15: f(x)= -0.427194\n",
      "differential_evolution step 16: f(x)= -0.427194\n",
      "differential_evolution step 17: f(x)= -0.427194\n",
      "differential_evolution step 18: f(x)= -0.427194\n",
      "differential_evolution step 19: f(x)= -0.427194\n",
      "differential_evolution step 20: f(x)= -0.427194\n",
      "variance optimization tolerance of changed to:  0.042719367806865444\n",
      "Next points to be requested: \n",
      "[[3.19231282e-03 9.99985154e+00]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  17\n",
      "Run Time:  0.6715927124023438      seconds\n",
      "Number of measurements:  17\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.042719367806865444\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0315461022238331\n",
      "Next points to be requested: \n",
      "[[7.88849896 1.18678418]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  18\n",
      "Run Time:  0.6765327453613281      seconds\n",
      "Number of measurements:  18\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0315461022238331\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.357553\n",
      "differential_evolution step 2: f(x)= -0.357553\n",
      "differential_evolution step 3: f(x)= -0.357553\n",
      "differential_evolution step 4: f(x)= -0.357553\n",
      "differential_evolution step 5: f(x)= -0.357553\n",
      "differential_evolution step 6: f(x)= -0.357553\n",
      "differential_evolution step 7: f(x)= -0.357553\n",
      "differential_evolution step 8: f(x)= -0.357553\n",
      "differential_evolution step 9: f(x)= -0.357553\n",
      "differential_evolution step 10: f(x)= -0.357553\n",
      "differential_evolution step 11: f(x)= -0.357553\n",
      "differential_evolution step 12: f(x)= -0.357553\n",
      "differential_evolution step 13: f(x)= -0.358181\n",
      "variance optimization tolerance of changed to:  0.03581813853290862\n",
      "Next points to be requested: \n",
      "[[9.98700048 6.08102977]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  19\n",
      "Run Time:  0.802114725112915      seconds\n",
      "Number of measurements:  19\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03581813853290862\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03160165293070922\n",
      "Next points to be requested: \n",
      "[[3.98984037 2.41663045]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  40  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [ 0.55454893 10.07998974 13.05908447]  with old log likelihood:  7.753074592077503\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local optimization not successful.\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  20\n",
      "Run Time:  0.8536782264709473      seconds\n",
      "Number of measurements:  20\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03160165293070922\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.34233\n",
      "differential_evolution step 2: f(x)= -0.350351\n",
      "differential_evolution step 3: f(x)= -0.350351\n",
      "differential_evolution step 4: f(x)= -0.354322\n",
      "differential_evolution step 5: f(x)= -0.354322\n",
      "differential_evolution step 6: f(x)= -0.354586\n",
      "differential_evolution step 7: f(x)= -0.354586\n",
      "differential_evolution step 8: f(x)= -0.354586\n",
      "differential_evolution step 9: f(x)= -0.354586\n",
      "differential_evolution step 10: f(x)= -0.354586\n",
      "differential_evolution step 11: f(x)= -0.354586\n",
      "differential_evolution step 12: f(x)= -0.354586\n",
      "differential_evolution step 13: f(x)= -0.354987\n",
      "differential_evolution step 14: f(x)= -0.355142\n",
      "differential_evolution step 15: f(x)= -0.355142\n",
      "variance optimization tolerance of changed to:  0.03551419028893273\n",
      "Next points to be requested: \n",
      "[[5.12910702 9.99916025]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  21\n",
      "Run Time:  1.0221748352050781      seconds\n",
      "Number of measurements:  21\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03551419028893273\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.026206097279507363\n",
      "Next points to be requested: \n",
      "[[4.40865436 7.5119102 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  22\n",
      "Run Time:  1.0282063484191895      seconds\n",
      "Number of measurements:  22\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.026206097279507363\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.339483\n",
      "differential_evolution step 2: f(x)= -0.343706\n",
      "differential_evolution step 3: f(x)= -0.343706\n",
      "differential_evolution step 4: f(x)= -0.344082\n",
      "differential_evolution step 5: f(x)= -0.346341\n",
      "differential_evolution step 6: f(x)= -0.346341\n",
      "differential_evolution step 7: f(x)= -0.347024\n",
      "differential_evolution step 8: f(x)= -0.347134\n",
      "differential_evolution step 9: f(x)= -0.347134\n",
      "differential_evolution step 10: f(x)= -0.347192\n",
      "differential_evolution step 11: f(x)= -0.347192\n",
      "differential_evolution step 12: f(x)= -0.347192\n",
      "differential_evolution step 13: f(x)= -0.347192\n",
      "variance optimization tolerance of changed to:  0.034719236962064554\n",
      "Next points to be requested: \n",
      "[[5.59441094e+00 6.08200186e-04]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  23\n",
      "Run Time:  1.1518378257751465      seconds\n",
      "Number of measurements:  23\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.034719236962064554\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02768663852556222\n",
      "Next points to be requested: \n",
      "[[5.7350753  3.65720886]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  24\n",
      "Run Time:  1.1580371856689453      seconds\n",
      "Number of measurements:  24\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02768663852556222\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.319903\n",
      "differential_evolution step 2: f(x)= -0.331241\n",
      "differential_evolution step 3: f(x)= -0.331241\n",
      "differential_evolution step 4: f(x)= -0.336378\n",
      "differential_evolution step 5: f(x)= -0.336378\n",
      "differential_evolution step 6: f(x)= -0.338141\n",
      "differential_evolution step 7: f(x)= -0.338141\n",
      "differential_evolution step 8: f(x)= -0.338726\n",
      "differential_evolution step 9: f(x)= -0.338968\n",
      "differential_evolution step 10: f(x)= -0.338968\n",
      "differential_evolution step 11: f(x)= -0.338968\n",
      "variance optimization tolerance of changed to:  0.03389681515781992\n",
      "Next points to be requested: \n",
      "[[0.01101461 3.18687187]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  25\n",
      "Run Time:  1.2619247436523438      seconds\n",
      "Number of measurements:  25\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03389681515781992\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.026338204871345278\n",
      "Next points to be requested: \n",
      "[[6.7028415 0.1710339]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  26\n",
      "Run Time:  1.2685511112213135      seconds\n",
      "Number of measurements:  26\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.026338204871345278\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.306779\n",
      "differential_evolution step 2: f(x)= -0.306779\n",
      "differential_evolution step 3: f(x)= -0.306779\n",
      "differential_evolution step 4: f(x)= -0.314057\n",
      "differential_evolution step 5: f(x)= -0.314057\n",
      "differential_evolution step 6: f(x)= -0.314057\n",
      "differential_evolution step 7: f(x)= -0.314057\n",
      "differential_evolution step 8: f(x)= -0.314057\n",
      "differential_evolution step 9: f(x)= -0.314057\n",
      "differential_evolution step 10: f(x)= -0.314057\n",
      "differential_evolution step 11: f(x)= -0.314057\n",
      "differential_evolution step 12: f(x)= -0.315211\n",
      "differential_evolution step 13: f(x)= -0.315211\n",
      "differential_evolution step 14: f(x)= -0.315543\n",
      "differential_evolution step 15: f(x)= -0.315543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 16: f(x)= -0.316045\n",
      "differential_evolution step 17: f(x)= -0.316045\n",
      "variance optimization tolerance of changed to:  0.03160445796809982\n",
      "Next points to be requested: \n",
      "[[9.99911217 2.63039536]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  27\n",
      "Run Time:  1.4745397567749023      seconds\n",
      "Number of measurements:  27\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03160445796809982\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02405434798458736\n",
      "Next points to be requested: \n",
      "[[4.57345043 3.39993212]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  28\n",
      "Run Time:  1.479637622833252      seconds\n",
      "Number of measurements:  28\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02405434798458736\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.290037\n",
      "differential_evolution step 2: f(x)= -0.291383\n",
      "differential_evolution step 3: f(x)= -0.302927\n",
      "differential_evolution step 4: f(x)= -0.302927\n",
      "differential_evolution step 5: f(x)= -0.305965\n",
      "differential_evolution step 6: f(x)= -0.308127\n",
      "differential_evolution step 7: f(x)= -0.308571\n",
      "differential_evolution step 8: f(x)= -0.309935\n",
      "differential_evolution step 9: f(x)= -0.309935\n",
      "differential_evolution step 10: f(x)= -0.311333\n",
      "variance optimization tolerance of changed to:  0.031133274857968643\n",
      "Next points to be requested: \n",
      "[[2.10029702 9.99116488]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  29\n",
      "Run Time:  1.5766749382019043      seconds\n",
      "Number of measurements:  29\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.031133274857968643\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02515615826837402\n",
      "Next points to be requested: \n",
      "[[1.94699836 8.37134453]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  30\n",
      "Run Time:  1.5827674865722656      seconds\n",
      "Number of measurements:  30\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02515615826837402\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.280418\n",
      "differential_evolution step 2: f(x)= -0.280653\n",
      "differential_evolution step 3: f(x)= -0.280713\n",
      "differential_evolution step 4: f(x)= -0.280713\n",
      "differential_evolution step 5: f(x)= -0.29905\n",
      "differential_evolution step 6: f(x)= -0.29905\n",
      "differential_evolution step 7: f(x)= -0.299522\n",
      "differential_evolution step 8: f(x)= -0.306747\n",
      "differential_evolution step 9: f(x)= -0.306747\n",
      "differential_evolution step 10: f(x)= -0.306747\n",
      "differential_evolution step 11: f(x)= -0.306839\n",
      "differential_evolution step 12: f(x)= -0.306839\n",
      "differential_evolution step 13: f(x)= -0.307121\n",
      "differential_evolution step 14: f(x)= -0.307121\n",
      "differential_evolution step 15: f(x)= -0.307121\n",
      "variance optimization tolerance of changed to:  0.03071209366446471\n",
      "Next points to be requested: \n",
      "[[7.5150106  9.99730329]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  31\n",
      "Run Time:  1.7549402713775635      seconds\n",
      "Number of measurements:  31\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03071209366446471\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02576824655325708\n",
      "Next points to be requested: \n",
      "[[9.82190615 8.85527088]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  32\n",
      "Run Time:  1.771695613861084      seconds\n",
      "Number of measurements:  32\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02576824655325708\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.269151\n",
      "differential_evolution step 2: f(x)= -0.272162\n",
      "differential_evolution step 3: f(x)= -0.272395\n",
      "differential_evolution step 4: f(x)= -0.272395\n",
      "differential_evolution step 5: f(x)= -0.272395\n",
      "differential_evolution step 6: f(x)= -0.27327\n",
      "differential_evolution step 7: f(x)= -0.27327\n",
      "differential_evolution step 8: f(x)= -0.27327\n",
      "variance optimization tolerance of changed to:  0.02732701363314712\n",
      "Next points to be requested: \n",
      "[[1.38141344 5.86460572]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  33\n",
      "Run Time:  1.8672091960906982      seconds\n",
      "Number of measurements:  33\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02732701363314712\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02222579600630481\n",
      "Next points to be requested: \n",
      "[[4.2840078  4.15978017]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  34\n",
      "Run Time:  1.87400221824646      seconds\n",
      "Number of measurements:  34\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02222579600630481\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.263937\n",
      "differential_evolution step 2: f(x)= -0.263937\n",
      "differential_evolution step 3: f(x)= -0.26643\n",
      "differential_evolution step 4: f(x)= -0.26643\n",
      "differential_evolution step 5: f(x)= -0.26643\n",
      "differential_evolution step 6: f(x)= -0.266734\n",
      "differential_evolution step 7: f(x)= -0.266815\n",
      "differential_evolution step 8: f(x)= -0.266926\n",
      "differential_evolution step 9: f(x)= -0.266978\n",
      "variance optimization tolerance of changed to:  0.026697750603410134\n",
      "Next points to be requested: \n",
      "[[5.90015773 5.60780922]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  35\n",
      "Run Time:  1.9705781936645508      seconds\n",
      "Number of measurements:  35\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026697750603410134\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021663669445420197\n",
      "Next points to be requested: \n",
      "[[1.27845934 3.69857458]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  36\n",
      "Run Time:  1.976780652999878      seconds\n",
      "Number of measurements:  36\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021663669445420197\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.254255\n",
      "differential_evolution step 2: f(x)= -0.258325\n",
      "differential_evolution step 3: f(x)= -0.259074\n",
      "differential_evolution step 4: f(x)= -0.259074\n",
      "differential_evolution step 5: f(x)= -0.259074\n",
      "differential_evolution step 6: f(x)= -0.259074\n",
      "differential_evolution step 7: f(x)= -0.259074\n",
      "differential_evolution step 8: f(x)= -0.259074\n",
      "differential_evolution step 9: f(x)= -0.259074\n",
      "differential_evolution step 10: f(x)= -0.259545\n",
      "differential_evolution step 11: f(x)= -0.25958\n",
      "differential_evolution step 12: f(x)= -0.259605\n",
      "differential_evolution step 13: f(x)= -0.260658\n",
      "differential_evolution step 14: f(x)= -0.260658\n",
      "differential_evolution step 15: f(x)= -0.260658\n",
      "differential_evolution step 16: f(x)= -0.260658\n",
      "differential_evolution step 17: f(x)= -0.261627\n",
      "differential_evolution step 18: f(x)= -0.26348\n",
      "differential_evolution step 19: f(x)= -0.26348\n",
      "variance optimization tolerance of changed to:  0.02634800257285701\n",
      "Next points to be requested: \n",
      "[[3.99730981 9.87253995]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  37\n",
      "Run Time:  2.167973518371582      seconds\n",
      "Number of measurements:  37\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02634800257285701\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.022668380576184647\n",
      "Next points to be requested: \n",
      "[[9.17668555 4.12704481]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  38\n",
      "Run Time:  2.1744298934936523      seconds\n",
      "Number of measurements:  38\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.022668380576184647\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.26193\n",
      "differential_evolution step 2: f(x)= -0.26193\n",
      "differential_evolution step 3: f(x)= -0.26193\n",
      "differential_evolution step 4: f(x)= -0.26193\n",
      "differential_evolution step 5: f(x)= -0.26193\n",
      "differential_evolution step 6: f(x)= -0.26193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 7: f(x)= -0.26193\n",
      "differential_evolution step 8: f(x)= -0.26193\n",
      "differential_evolution step 9: f(x)= -0.26193\n",
      "differential_evolution step 10: f(x)= -0.26193\n",
      "differential_evolution step 11: f(x)= -0.26193\n",
      "differential_evolution step 12: f(x)= -0.262194\n",
      "differential_evolution step 13: f(x)= -0.262194\n",
      "differential_evolution step 14: f(x)= -0.262194\n",
      "differential_evolution step 15: f(x)= -0.262194\n",
      "differential_evolution step 16: f(x)= -0.262194\n",
      "differential_evolution step 17: f(x)= -0.262194\n",
      "differential_evolution step 18: f(x)= -0.262194\n",
      "differential_evolution step 19: f(x)= -0.262194\n",
      "differential_evolution step 20: f(x)= -0.262194\n",
      "variance optimization tolerance of changed to:  0.0262194225235956\n",
      "Next points to be requested: \n",
      "[[0.04186884 5.05234487]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "GPOptimizer updated the Hyperperameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  39\n",
      "Run Time:  2.417207717895508      seconds\n",
      "Number of measurements:  39\n",
      "====================\n",
      "hps:  [ 0.55454893 10.07998974 13.05908447]\n",
      "aks() initiated with hyperparameters: [ 0.55454893 10.07998974 13.05908447]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0262194225235956\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.020174948479146058\n",
      "Next points to be requested: \n",
      "[[4.70070536 4.62013419]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  80  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [ 0.55454893 10.07998974 13.05908447]  with old log likelihood:  7.59547286059049\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [ 0.55454893 10.07998974 13.05908447]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization successfully concluded with result:  -23.04723667968716\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  40\n",
      "Run Time:  2.505925416946411      seconds\n",
      "Number of measurements:  40\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.020174948479146058\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.552031\n",
      "differential_evolution step 2: f(x)= -0.563369\n",
      "differential_evolution step 3: f(x)= -0.563369\n",
      "differential_evolution step 4: f(x)= -0.563369\n",
      "differential_evolution step 5: f(x)= -0.563369\n",
      "differential_evolution step 6: f(x)= -0.563556\n",
      "differential_evolution step 7: f(x)= -0.563556\n",
      "differential_evolution step 8: f(x)= -0.563726\n",
      "variance optimization tolerance of changed to:  0.0563726417037396\n",
      "Next points to be requested: \n",
      "[[1.43880572 1.80691777]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  41\n",
      "Run Time:  2.6077218055725098      seconds\n",
      "Number of measurements:  41\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0563726417037396\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05168005047672134\n",
      "Next points to be requested: \n",
      "[[5.4820458  1.01356738]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  42\n",
      "Run Time:  2.623034715652466      seconds\n",
      "Number of measurements:  42\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05168005047672134\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.554587\n",
      "differential_evolution step 2: f(x)= -0.554587\n",
      "differential_evolution step 3: f(x)= -0.554587\n",
      "variance optimization tolerance of changed to:  0.055458695292658455\n",
      "Next points to be requested: \n",
      "[[0.12679204 8.46589007]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  43\n",
      "Run Time:  2.685676097869873      seconds\n",
      "Number of measurements:  43\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.055458695292658455\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04733096388733943\n",
      "Next points to be requested: \n",
      "[[0.62980445 2.49170637]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  44\n",
      "Run Time:  2.693748950958252      seconds\n",
      "Number of measurements:  44\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04733096388733943\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.541821\n",
      "differential_evolution step 2: f(x)= -0.541821\n",
      "differential_evolution step 3: f(x)= -0.541882\n",
      "differential_evolution step 4: f(x)= -0.541882\n",
      "variance optimization tolerance of changed to:  0.054188248685777096\n",
      "Next points to be requested: \n",
      "[[9.59396488 7.41347124]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  45\n",
      "Run Time:  2.7477540969848633      seconds\n",
      "Number of measurements:  45\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.054188248685777096\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0497074028685617\n",
      "Next points to be requested: \n",
      "[[7.42295365 2.99301499]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  46\n",
      "Run Time:  2.754751205444336      seconds\n",
      "Number of measurements:  46\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0497074028685617\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.535681\n",
      "differential_evolution step 2: f(x)= -0.535681\n",
      "differential_evolution step 3: f(x)= -0.535681\n",
      "variance optimization tolerance of changed to:  0.05356812022123161\n",
      "Next points to be requested: \n",
      "[[8.51272638 0.08382346]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  47\n",
      "Run Time:  2.798196792602539      seconds\n",
      "Number of measurements:  47\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05356812022123161\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.047245580388169256\n",
      "Next points to be requested: \n",
      "[[0.85907548 7.95728075]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  48\n",
      "Run Time:  2.8058815002441406      seconds\n",
      "Number of measurements:  48\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.047245580388169256\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.519553\n",
      "differential_evolution step 2: f(x)= -0.534167\n",
      "differential_evolution step 3: f(x)= -0.534167\n",
      "differential_evolution step 4: f(x)= -0.534167\n",
      "differential_evolution step 5: f(x)= -0.534167\n",
      "variance optimization tolerance of changed to:  0.053416667529264675\n",
      "Next points to be requested: \n",
      "[[2.93144001 6.83178798]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  49\n",
      "Run Time:  2.8730485439300537      seconds\n",
      "Number of measurements:  49\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.053416667529264675\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.05122919686463715\n",
      "Next points to be requested: \n",
      "[[6.58750175 9.13476753]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  50\n",
      "Run Time:  2.8794639110565186      seconds\n",
      "Number of measurements:  50\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.05122919686463715\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.516073\n",
      "differential_evolution step 2: f(x)= -0.516794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 3: f(x)= -0.516794\n",
      "variance optimization tolerance of changed to:  0.051679392360157506\n",
      "Next points to be requested: \n",
      "[[2.75556476 2.03504763]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  51\n",
      "Run Time:  2.9264919757843018      seconds\n",
      "Number of measurements:  51\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.051679392360157506\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04532406411494641\n",
      "Next points to be requested: \n",
      "[[2.09195396 6.32097454]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  52\n",
      "Run Time:  2.935734987258911      seconds\n",
      "Number of measurements:  52\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04532406411494641\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.529276\n",
      "differential_evolution step 2: f(x)= -0.529276\n",
      "differential_evolution step 3: f(x)= -0.529276\n",
      "differential_evolution step 4: f(x)= -0.529276\n",
      "differential_evolution step 5: f(x)= -0.529276\n",
      "differential_evolution step 6: f(x)= -0.529276\n",
      "differential_evolution step 7: f(x)= -0.529276\n",
      "differential_evolution step 8: f(x)= -0.529276\n",
      "variance optimization tolerance of changed to:  0.05292756123381229\n",
      "Next points to be requested: \n",
      "[[9.92404597 1.33628755]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  53\n",
      "Run Time:  3.033844470977783      seconds\n",
      "Number of measurements:  53\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05292756123381229\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04398378448603434\n",
      "Next points to be requested: \n",
      "[[6.45719058 8.43676852]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  54\n",
      "Run Time:  3.0410099029541016      seconds\n",
      "Number of measurements:  54\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04398378448603434\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.508069\n",
      "differential_evolution step 2: f(x)= -0.512218\n",
      "differential_evolution step 3: f(x)= -0.512218\n",
      "differential_evolution step 4: f(x)= -0.514885\n",
      "differential_evolution step 5: f(x)= -0.514885\n",
      "variance optimization tolerance of changed to:  0.051488541804247626\n",
      "Next points to be requested: \n",
      "[[4.50906436 6.11305447]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  55\n",
      "Run Time:  3.108086585998535      seconds\n",
      "Number of measurements:  55\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.051488541804247626\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04678535492408098\n",
      "Next points to be requested: \n",
      "[[7.00048123 1.34166885]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  56\n",
      "Run Time:  3.1164236068725586      seconds\n",
      "Number of measurements:  56\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04678535492408098\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.50329\n",
      "differential_evolution step 2: f(x)= -0.511706\n",
      "differential_evolution step 3: f(x)= -0.511706\n",
      "differential_evolution step 4: f(x)= -0.511706\n",
      "variance optimization tolerance of changed to:  0.05117060460049194\n",
      "Next points to be requested: \n",
      "[[8.62035422 6.43994364]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  57\n",
      "Run Time:  3.21802020072937      seconds\n",
      "Number of measurements:  57\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.05117060460049194\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04255254157177671\n",
      "Next points to be requested: \n",
      "[[4.47789178 5.38973801]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  58\n",
      "Run Time:  3.2255263328552246      seconds\n",
      "Number of measurements:  58\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04255254157177671\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.513133\n",
      "differential_evolution step 2: f(x)= -0.513133\n",
      "differential_evolution step 3: f(x)= -0.514791\n",
      "differential_evolution step 4: f(x)= -0.514791\n",
      "differential_evolution step 5: f(x)= -0.514791\n",
      "variance optimization tolerance of changed to:  0.051479097158122294\n",
      "Next points to be requested: \n",
      "[[4.12538134 0.04503411]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "GPOptimizer updated the Hyperperameters:  [0.43457632 1.70106604 1.63277365]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  59\n",
      "Run Time:  3.29474139213562      seconds\n",
      "Number of measurements:  59\n",
      "====================\n",
      "hps:  [0.43457632 1.70106604 1.63277365]\n",
      "aks() initiated with hyperparameters: [0.43457632 1.70106604 1.63277365]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.051479097158122294\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04117456636707032\n",
      "Next points to be requested: \n",
      "[[6.1882292 0.859546 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  120  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.43457632 1.70106604 1.63277365]  with old log likelihood:  -37.602206741722384\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.43457632 1.70106604 1.63277365]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization successfully concluded with result:  -39.35394914549612\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  60\n",
      "Run Time:  4.064048767089844      seconds\n",
      "Number of measurements:  60\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04117456636707032\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.427874\n",
      "differential_evolution step 2: f(x)= -0.428574\n",
      "differential_evolution step 3: f(x)= -0.437416\n",
      "differential_evolution step 4: f(x)= -0.437416\n",
      "differential_evolution step 5: f(x)= -0.437416\n",
      "differential_evolution step 6: f(x)= -0.438179\n",
      "differential_evolution step 7: f(x)= -0.438179\n",
      "differential_evolution step 8: f(x)= -0.438905\n",
      "variance optimization tolerance of changed to:  0.04389051863217696\n",
      "Next points to be requested: \n",
      "[[6.94853752 4.55080085]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  61\n",
      "Run Time:  4.2186150550842285      seconds\n",
      "Number of measurements:  61\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04389051863217696\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.032093570782840496\n",
      "Next points to be requested: \n",
      "[[2.46524316 0.36485818]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  62\n",
      "Run Time:  4.226797580718994      seconds\n",
      "Number of measurements:  62\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.032093570782840496\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.419232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 2: f(x)= -0.419402\n",
      "differential_evolution step 3: f(x)= -0.431705\n",
      "differential_evolution step 4: f(x)= -0.431705\n",
      "differential_evolution step 5: f(x)= -0.441793\n",
      "differential_evolution step 6: f(x)= -0.441793\n",
      "differential_evolution step 7: f(x)= -0.441793\n",
      "differential_evolution step 8: f(x)= -0.444552\n",
      "differential_evolution step 9: f(x)= -0.445997\n",
      "differential_evolution step 10: f(x)= -0.445997\n",
      "differential_evolution step 11: f(x)= -0.445997\n",
      "differential_evolution step 12: f(x)= -0.445997\n",
      "differential_evolution step 13: f(x)= -0.445997\n",
      "differential_evolution step 14: f(x)= -0.446555\n",
      "differential_evolution step 15: f(x)= -0.446555\n",
      "differential_evolution step 16: f(x)= -0.446555\n",
      "differential_evolution step 17: f(x)= -0.446555\n",
      "differential_evolution step 18: f(x)= -0.446555\n",
      "variance optimization tolerance of changed to:  0.04465550839062994\n",
      "Next points to be requested: \n",
      "[[8.70550678 9.98698497]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  63\n",
      "Run Time:  4.494516849517822      seconds\n",
      "Number of measurements:  63\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04465550839062994\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0343596971224031\n",
      "Next points to be requested: \n",
      "[[4.57251321 6.83502098]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  64\n",
      "Run Time:  4.504188776016235      seconds\n",
      "Number of measurements:  64\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0343596971224031\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.430157\n",
      "differential_evolution step 2: f(x)= -0.430157\n",
      "differential_evolution step 3: f(x)= -0.450853\n",
      "differential_evolution step 4: f(x)= -0.450853\n",
      "differential_evolution step 5: f(x)= -0.450853\n",
      "differential_evolution step 6: f(x)= -0.450853\n",
      "differential_evolution step 7: f(x)= -0.453949\n",
      "differential_evolution step 8: f(x)= -0.45402\n",
      "differential_evolution step 9: f(x)= -0.45494\n",
      "differential_evolution step 10: f(x)= -0.45494\n",
      "variance optimization tolerance of changed to:  0.04549398554095181\n",
      "Next points to be requested: \n",
      "[[0.01496973 1.43895862]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  65\n",
      "Run Time:  4.63935112953186      seconds\n",
      "Number of measurements:  65\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04549398554095181\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.04116880322357779\n",
      "Next points to be requested: \n",
      "[[3.05565519 4.20468358]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  66\n",
      "Run Time:  4.647699356079102      seconds\n",
      "Number of measurements:  66\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.04116880322357779\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.416729\n",
      "differential_evolution step 2: f(x)= -0.416729\n",
      "differential_evolution step 3: f(x)= -0.416729\n",
      "differential_evolution step 4: f(x)= -0.418958\n",
      "differential_evolution step 5: f(x)= -0.418958\n",
      "variance optimization tolerance of changed to:  0.04189580793653955\n",
      "Next points to be requested: \n",
      "[[4.52608396 8.72848882]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  67\n",
      "Run Time:  4.726175308227539      seconds\n",
      "Number of measurements:  67\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04189580793653955\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03141016865614157\n",
      "Next points to be requested: \n",
      "[[7.01020681 8.74472127]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  68\n",
      "Run Time:  4.735330104827881      seconds\n",
      "Number of measurements:  68\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03141016865614157\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.410268\n",
      "differential_evolution step 2: f(x)= -0.410268\n",
      "differential_evolution step 3: f(x)= -0.410268\n",
      "differential_evolution step 4: f(x)= -0.410268\n",
      "differential_evolution step 5: f(x)= -0.419005\n",
      "differential_evolution step 6: f(x)= -0.426849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 7: f(x)= -0.426849\n",
      "differential_evolution step 8: f(x)= -0.426869\n",
      "differential_evolution step 9: f(x)= -0.426869\n",
      "differential_evolution step 10: f(x)= -0.4271\n",
      "differential_evolution step 11: f(x)= -0.4271\n",
      "differential_evolution step 12: f(x)= -0.4271\n",
      "variance optimization tolerance of changed to:  0.04270995483031274\n",
      "Next points to be requested: \n",
      "[[8.71501896 2.17180824]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  69\n",
      "Run Time:  4.941624164581299      seconds\n",
      "Number of measurements:  69\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04270995483031274\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03611570742844085\n",
      "Next points to be requested: \n",
      "[[2.27743397 9.29687774]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  70\n",
      "Run Time:  4.95002007484436      seconds\n",
      "Number of measurements:  70\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03611570742844085\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.410889\n",
      "differential_evolution step 2: f(x)= -0.410889\n",
      "differential_evolution step 3: f(x)= -0.411831\n",
      "differential_evolution step 4: f(x)= -0.411831\n",
      "differential_evolution step 5: f(x)= -0.411831\n",
      "differential_evolution step 6: f(x)= -0.411831\n",
      "differential_evolution step 7: f(x)= -0.434791\n",
      "differential_evolution step 8: f(x)= -0.440446\n",
      "differential_evolution step 9: f(x)= -0.440446\n",
      "differential_evolution step 10: f(x)= -0.440446\n",
      "differential_evolution step 11: f(x)= -0.440446\n",
      "differential_evolution step 12: f(x)= -0.440446\n",
      "differential_evolution step 13: f(x)= -0.444548\n",
      "differential_evolution step 14: f(x)= -0.444548\n",
      "differential_evolution step 15: f(x)= -0.445116\n",
      "differential_evolution step 16: f(x)= -0.445116\n",
      "differential_evolution step 17: f(x)= -0.445116\n",
      "differential_evolution step 18: f(x)= -0.445116\n",
      "variance optimization tolerance of changed to:  0.0445115753981134\n",
      "Next points to be requested: \n",
      "[[9.99679648 4.75218659]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  71\n",
      "Run Time:  5.213626384735107      seconds\n",
      "Number of measurements:  71\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0445115753981134\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.033849650237292105\n",
      "Next points to be requested: \n",
      "[[3.86677367 5.27208702]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  72\n",
      "Run Time:  5.222218036651611      seconds\n",
      "Number of measurements:  72\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.033849650237292105\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.40742\n",
      "differential_evolution step 2: f(x)= -0.408922\n",
      "differential_evolution step 3: f(x)= -0.410054\n",
      "differential_evolution step 4: f(x)= -0.410054\n",
      "differential_evolution step 5: f(x)= -0.411062\n",
      "differential_evolution step 6: f(x)= -0.411062\n",
      "differential_evolution step 7: f(x)= -0.411898\n",
      "differential_evolution step 8: f(x)= -0.411963\n",
      "differential_evolution step 9: f(x)= -0.412088\n",
      "differential_evolution step 10: f(x)= -0.412088\n",
      "variance optimization tolerance of changed to:  0.041208795417654216\n",
      "Next points to be requested: \n",
      "[[8.67534585 8.54653325]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  73\n",
      "Run Time:  5.420076608657837      seconds\n",
      "Number of measurements:  73\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.041208795417654216\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03653815239384072\n",
      "Next points to be requested: \n",
      "[[5.98444572 4.3757527 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  74\n",
      "Run Time:  5.428903341293335      seconds\n",
      "Number of measurements:  74\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03653815239384072\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.406749\n",
      "differential_evolution step 2: f(x)= -0.406749\n",
      "differential_evolution step 3: f(x)= -0.414256\n",
      "differential_evolution step 4: f(x)= -0.414256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 5: f(x)= -0.414256\n",
      "differential_evolution step 6: f(x)= -0.414256\n",
      "differential_evolution step 7: f(x)= -0.414256\n",
      "differential_evolution step 8: f(x)= -0.414256\n",
      "differential_evolution step 9: f(x)= -0.414261\n",
      "differential_evolution step 10: f(x)= -0.414261\n",
      "variance optimization tolerance of changed to:  0.04142609802457474\n",
      "Next points to be requested: \n",
      "[[4.17539231 1.23264628]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  75\n",
      "Run Time:  5.586132049560547      seconds\n",
      "Number of measurements:  75\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04142609802457474\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.033054617752029586\n",
      "Next points to be requested: \n",
      "[[9.37830023 9.86731113]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  76\n",
      "Run Time:  5.597029209136963      seconds\n",
      "Number of measurements:  76\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.033054617752029586\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.390677\n",
      "differential_evolution step 2: f(x)= -0.390677\n",
      "differential_evolution step 3: f(x)= -0.401025\n",
      "differential_evolution step 4: f(x)= -0.402684\n",
      "differential_evolution step 5: f(x)= -0.403353\n",
      "differential_evolution step 6: f(x)= -0.403353\n",
      "differential_evolution step 7: f(x)= -0.407468\n",
      "differential_evolution step 8: f(x)= -0.407468\n",
      "differential_evolution step 9: f(x)= -0.40762\n",
      "differential_evolution step 10: f(x)= -0.40762\n",
      "differential_evolution step 11: f(x)= -0.408717\n",
      "differential_evolution step 12: f(x)= -0.409711\n",
      "variance optimization tolerance of changed to:  0.04097107322043549\n",
      "Next points to be requested: \n",
      "[[7.1102971 5.9255909]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  77\n",
      "Run Time:  5.883269786834717      seconds\n",
      "Number of measurements:  77\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.04097107322043549\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03815727411560013\n",
      "Next points to be requested: \n",
      "[[0.22584052 4.05131893]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  78\n",
      "Run Time:  5.894562482833862      seconds\n",
      "Number of measurements:  78\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03815727411560013\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.393677\n",
      "differential_evolution step 2: f(x)= -0.393677\n",
      "differential_evolution step 3: f(x)= -0.393677\n",
      "differential_evolution step 4: f(x)= -0.39475\n",
      "differential_evolution step 5: f(x)= -0.399744\n",
      "differential_evolution step 6: f(x)= -0.399744\n",
      "differential_evolution step 7: f(x)= -0.399744\n",
      "differential_evolution step 8: f(x)= -0.399744\n",
      "variance optimization tolerance of changed to:  0.039974410722382336\n",
      "Next points to be requested: \n",
      "[[5.31535765 2.58522961]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "GPOptimizer updated the Hyperperameters:  [0.60418814 3.42509693 3.51637112]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  79\n",
      "Run Time:  6.048566102981567      seconds\n",
      "Number of measurements:  79\n",
      "====================\n",
      "hps:  [0.60418814 3.42509693 3.51637112]\n",
      "aks() initiated with hyperparameters: [0.60418814 3.42509693 3.51637112]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.039974410722382336\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03702463176034045\n",
      "Next points to be requested: \n",
      "[[3.03693716 1.14553918]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  160  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.60418814 3.42509693 3.51637112]  with old log likelihood:  -64.23019756128343\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.60418814 3.42509693 3.51637112]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local optimization successfully concluded with result:  -65.34231777595798\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  80\n",
      "Run Time:  7.123771667480469      seconds\n",
      "Number of measurements:  80\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03702463176034045\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.357709\n",
      "differential_evolution step 2: f(x)= -0.357709\n",
      "differential_evolution step 3: f(x)= -0.357709\n",
      "differential_evolution step 4: f(x)= -0.357709\n",
      "differential_evolution step 5: f(x)= -0.359085\n",
      "differential_evolution step 6: f(x)= -0.359085\n",
      "differential_evolution step 7: f(x)= -0.360756\n",
      "differential_evolution step 8: f(x)= -0.360756\n",
      "differential_evolution step 9: f(x)= -0.364331\n",
      "differential_evolution step 10: f(x)= -0.364331\n",
      "differential_evolution step 11: f(x)= -0.364331\n",
      "differential_evolution step 12: f(x)= -0.364331\n",
      "differential_evolution step 13: f(x)= -0.364331\n",
      "differential_evolution step 14: f(x)= -0.365179\n",
      "differential_evolution step 15: f(x)= -0.365179\n",
      "variance optimization tolerance of changed to:  0.03651787741994591\n",
      "Next points to be requested: \n",
      "[[1.06983968 9.9841568 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  81\n",
      "Run Time:  7.390413761138916      seconds\n",
      "Number of measurements:  81\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03651787741994591\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02805302476691464\n",
      "Next points to be requested: \n",
      "[[3.57503502 4.68801713]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  82\n",
      "Run Time:  7.4027485847473145      seconds\n",
      "Number of measurements:  82\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02805302476691464\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.343268\n",
      "differential_evolution step 2: f(x)= -0.346153\n",
      "differential_evolution step 3: f(x)= -0.346153\n",
      "differential_evolution step 4: f(x)= -0.346578\n",
      "differential_evolution step 5: f(x)= -0.347927\n",
      "differential_evolution step 6: f(x)= -0.349365\n",
      "differential_evolution step 7: f(x)= -0.349365\n",
      "differential_evolution step 8: f(x)= -0.349812\n",
      "differential_evolution step 9: f(x)= -0.349812\n",
      "differential_evolution step 10: f(x)= -0.349812\n",
      "variance optimization tolerance of changed to:  0.034981226730603755\n",
      "Next points to be requested: \n",
      "[[1.68443218 4.82293689]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  83\n",
      "Run Time:  7.591032028198242      seconds\n",
      "Number of measurements:  83\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.034981226730603755\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.028404403953318915\n",
      "Next points to be requested: \n",
      "[[6.12207676 5.09554724]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  84\n",
      "Run Time:  7.602149724960327      seconds\n",
      "Number of measurements:  84\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.028404403953318915\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.340714\n",
      "differential_evolution step 2: f(x)= -0.351648\n",
      "differential_evolution step 3: f(x)= -0.351648\n",
      "differential_evolution step 4: f(x)= -0.351648\n",
      "differential_evolution step 5: f(x)= -0.351648\n",
      "differential_evolution step 6: f(x)= -0.356957\n",
      "differential_evolution step 7: f(x)= -0.356957\n",
      "differential_evolution step 8: f(x)= -0.356957\n",
      "differential_evolution step 9: f(x)= -0.356957\n",
      "differential_evolution step 10: f(x)= -0.356957\n",
      "differential_evolution step 11: f(x)= -0.356957\n",
      "differential_evolution step 12: f(x)= -0.356957\n",
      "differential_evolution step 13: f(x)= -0.356957\n",
      "differential_evolution step 14: f(x)= -0.356957\n",
      "differential_evolution step 15: f(x)= -0.356957\n",
      "differential_evolution step 16: f(x)= -0.356957\n",
      "differential_evolution step 17: f(x)= -0.356957\n",
      "differential_evolution step 18: f(x)= -0.356957\n",
      "differential_evolution step 19: f(x)= -0.356957\n",
      "differential_evolution step 20: f(x)= -0.356957\n",
      "variance optimization tolerance of changed to:  0.03569573925448758\n",
      "Next points to be requested: \n",
      "[[1.02619699 0.00257122]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  85\n",
      "Run Time:  8.008249044418335      seconds\n",
      "Number of measurements:  85\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03569573925448758\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.028766514009723687\n",
      "Next points to be requested: \n",
      "[[8.00670603 3.26889966]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  86\n",
      "Run Time:  8.045644044876099      seconds\n",
      "Number of measurements:  86\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.028766514009723687\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.336266\n",
      "differential_evolution step 2: f(x)= -0.336266\n",
      "differential_evolution step 3: f(x)= -0.344308\n",
      "differential_evolution step 4: f(x)= -0.345127\n",
      "differential_evolution step 5: f(x)= -0.345127\n",
      "differential_evolution step 6: f(x)= -0.345127\n",
      "differential_evolution step 7: f(x)= -0.345127\n",
      "differential_evolution step 8: f(x)= -0.345127\n",
      "differential_evolution step 9: f(x)= -0.345127\n",
      "differential_evolution step 10: f(x)= -0.345127\n",
      "differential_evolution step 11: f(x)= -0.345127\n",
      "differential_evolution step 12: f(x)= -0.355173\n",
      "differential_evolution step 13: f(x)= -0.355173\n",
      "differential_evolution step 14: f(x)= -0.355173\n",
      "differential_evolution step 15: f(x)= -0.358491\n",
      "differential_evolution step 16: f(x)= -0.358491\n",
      "differential_evolution step 17: f(x)= -0.358491\n",
      "differential_evolution step 18: f(x)= -0.358964\n",
      "differential_evolution step 19: f(x)= -0.359008\n",
      "differential_evolution step 20: f(x)= -0.361076\n",
      "variance optimization tolerance of changed to:  0.036107637524446116\n",
      "Next points to be requested: \n",
      "[[6.17157941 9.99860115]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  87\n",
      "Run Time:  8.438464879989624      seconds\n",
      "Number of measurements:  87\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.036107637524446116\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03357551467609569\n",
      "Next points to be requested: \n",
      "[[9.80659745 3.42987876]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  88\n",
      "Run Time:  8.447873830795288      seconds\n",
      "Number of measurements:  88\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03357551467609569\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.34842\n",
      "differential_evolution step 2: f(x)= -0.34842\n",
      "differential_evolution step 3: f(x)= -0.34842\n",
      "differential_evolution step 4: f(x)= -0.34842\n",
      "differential_evolution step 5: f(x)= -0.34842\n",
      "differential_evolution step 6: f(x)= -0.34842\n",
      "differential_evolution step 7: f(x)= -0.34842\n",
      "differential_evolution step 8: f(x)= -0.34842\n",
      "differential_evolution step 9: f(x)= -0.34842\n",
      "differential_evolution step 10: f(x)= -0.34842\n",
      "differential_evolution step 11: f(x)= -0.348807\n",
      "differential_evolution step 12: f(x)= -0.348807\n",
      "variance optimization tolerance of changed to:  0.034880651305929204\n",
      "Next points to be requested: \n",
      "[[3.00698831 3.12964351]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  89\n",
      "Run Time:  8.743964433670044      seconds\n",
      "Number of measurements:  89\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.034880651305929204\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03311531424022229\n",
      "Next points to be requested: \n",
      "[[8.4169098  7.33685766]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  90\n",
      "Run Time:  8.754308462142944      seconds\n",
      "Number of measurements:  90\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03311531424022229\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.343467\n",
      "differential_evolution step 2: f(x)= -0.343467\n",
      "differential_evolution step 3: f(x)= -0.344799\n",
      "differential_evolution step 4: f(x)= -0.344799\n",
      "differential_evolution step 5: f(x)= -0.344799\n",
      "differential_evolution step 6: f(x)= -0.344799\n",
      "differential_evolution step 7: f(x)= -0.344799\n",
      "variance optimization tolerance of changed to:  0.034479917254401256\n",
      "Next points to be requested: \n",
      "[[1.46113437 7.12490989]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  91\n",
      "Run Time:  8.897131204605103      seconds\n",
      "Number of measurements:  91\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.034479917254401256\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.030394151711035346\n",
      "Next points to be requested: \n",
      "[[1.1604712  2.87775485]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  92\n",
      "Run Time:  8.913986444473267      seconds\n",
      "Number of measurements:  92\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.030394151711035346\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.326922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 2: f(x)= -0.334548\n",
      "differential_evolution step 3: f(x)= -0.334548\n",
      "differential_evolution step 4: f(x)= -0.341142\n",
      "differential_evolution step 5: f(x)= -0.341142\n",
      "differential_evolution step 6: f(x)= -0.341142\n",
      "differential_evolution step 7: f(x)= -0.341142\n",
      "differential_evolution step 8: f(x)= -0.341142\n",
      "differential_evolution step 9: f(x)= -0.341142\n",
      "differential_evolution step 10: f(x)= -0.341142\n",
      "differential_evolution step 11: f(x)= -0.341142\n",
      "differential_evolution step 12: f(x)= -0.341142\n",
      "differential_evolution step 13: f(x)= -0.341972\n",
      "differential_evolution step 14: f(x)= -0.341972\n",
      "differential_evolution step 15: f(x)= -0.341972\n",
      "variance optimization tolerance of changed to:  0.0341972011554639\n",
      "Next points to be requested: \n",
      "[[5.87074349 6.69588955]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  93\n",
      "Run Time:  9.2698655128479      seconds\n",
      "Number of measurements:  93\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0341972011554639\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.027384616134335977\n",
      "Next points to be requested: \n",
      "[[8.94992487 7.29663227]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  94\n",
      "Run Time:  9.281192779541016      seconds\n",
      "Number of measurements:  94\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.027384616134335977\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.321163\n",
      "differential_evolution step 2: f(x)= -0.324776\n",
      "differential_evolution step 3: f(x)= -0.324776\n",
      "differential_evolution step 4: f(x)= -0.324776\n",
      "differential_evolution step 5: f(x)= -0.324776\n",
      "differential_evolution step 6: f(x)= -0.324776\n",
      "differential_evolution step 7: f(x)= -0.3327\n",
      "differential_evolution step 8: f(x)= -0.3327\n",
      "differential_evolution step 9: f(x)= -0.3327\n",
      "differential_evolution step 10: f(x)= -0.3327\n",
      "differential_evolution step 11: f(x)= -0.334197\n",
      "differential_evolution step 12: f(x)= -0.336956\n",
      "differential_evolution step 13: f(x)= -0.336956\n",
      "differential_evolution step 14: f(x)= -0.338686\n",
      "differential_evolution step 15: f(x)= -0.338686\n",
      "differential_evolution step 16: f(x)= -0.338686\n",
      "differential_evolution step 17: f(x)= -0.338686\n",
      "differential_evolution step 18: f(x)= -0.338686\n",
      "variance optimization tolerance of changed to:  0.03386857051462174\n",
      "Next points to be requested: \n",
      "[[3.09282816 9.86341638]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  95\n",
      "Run Time:  9.69641637802124      seconds\n",
      "Number of measurements:  95\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03386857051462174\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.030417943709590917\n",
      "Next points to be requested: \n",
      "[[5.90948756 8.70085317]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  96\n",
      "Run Time:  9.708563804626465      seconds\n",
      "Number of measurements:  96\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.030417943709590917\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.316456\n",
      "differential_evolution step 2: f(x)= -0.32276\n",
      "differential_evolution step 3: f(x)= -0.32276\n",
      "differential_evolution step 4: f(x)= -0.32276\n",
      "differential_evolution step 5: f(x)= -0.32276\n",
      "differential_evolution step 6: f(x)= -0.328413\n",
      "differential_evolution step 7: f(x)= -0.331677\n",
      "differential_evolution step 8: f(x)= -0.331677\n",
      "differential_evolution step 9: f(x)= -0.332817\n",
      "variance optimization tolerance of changed to:  0.03328169900582037\n",
      "Next points to be requested: \n",
      "[[9.13465563 5.56473804]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  97\n",
      "Run Time:  9.897643089294434      seconds\n",
      "Number of measurements:  97\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03328169900582037\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.028856477288112792\n",
      "Next points to be requested: \n",
      "[[8.11882726 6.74870881]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  98\n",
      "Run Time:  9.910475254058838      seconds\n",
      "Number of measurements:  98\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.028856477288112792\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.329635\n",
      "differential_evolution step 2: f(x)= -0.335797\n",
      "differential_evolution step 3: f(x)= -0.335797\n",
      "differential_evolution step 4: f(x)= -0.335797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 5: f(x)= -0.335797\n",
      "differential_evolution step 6: f(x)= -0.335797\n",
      "differential_evolution step 7: f(x)= -0.335797\n",
      "differential_evolution step 8: f(x)= -0.335797\n",
      "differential_evolution step 9: f(x)= -0.336784\n",
      "differential_evolution step 10: f(x)= -0.336784\n",
      "differential_evolution step 11: f(x)= -0.336891\n",
      "differential_evolution step 12: f(x)= -0.338491\n",
      "differential_evolution step 13: f(x)= -0.339793\n",
      "differential_evolution step 14: f(x)= -0.339793\n",
      "differential_evolution step 15: f(x)= -0.339793\n",
      "differential_evolution step 16: f(x)= -0.339793\n",
      "variance optimization tolerance of changed to:  0.03397929907661359\n",
      "Next points to be requested: \n",
      "[[2.95638674e-04 5.92261055e+00]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.72235341 5.43848105 5.38444706]\n",
      "GPOptimizer updated the Hyperperameters:  [0.72235341 5.43848105 5.38444706]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  99\n",
      "Run Time:  10.272634744644165      seconds\n",
      "Number of measurements:  99\n",
      "====================\n",
      "hps:  [0.72235341 5.43848105 5.38444706]\n",
      "aks() initiated with hyperparameters: [0.72235341 5.43848105 5.38444706]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.03397929907661359\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.03229603366955793\n",
      "Next points to be requested: \n",
      "[[0.64602349 0.99302875]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  200  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.72235341 5.43848105 5.38444706]  with old log likelihood:  -92.71114546651151\n",
      "method:  global\n",
      "I am performing a global differential evolution algorithm to find the optimal hyperparameters.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "differential_evolution step 1: f(x)= -89.5666\n",
      "differential_evolution step 2: f(x)= -91.2163\n",
      "differential_evolution step 3: f(x)= -91.9056\n",
      "differential_evolution step 4: f(x)= -91.9056\n",
      "differential_evolution step 5: f(x)= -91.9056\n",
      "differential_evolution step 6: f(x)= -91.9609\n",
      "differential_evolution step 7: f(x)= -91.9609\n",
      "differential_evolution step 8: f(x)= -91.9609\n",
      "differential_evolution step 9: f(x)= -92.1599\n",
      "differential_evolution step 10: f(x)= -92.1996\n",
      "differential_evolution step 11: f(x)= -92.5666\n",
      "differential_evolution step 12: f(x)= -92.9865\n",
      "differential_evolution step 13: f(x)= -93.2146\n",
      "differential_evolution step 14: f(x)= -93.2146\n",
      "differential_evolution step 15: f(x)= -93.2146\n",
      "differential_evolution step 16: f(x)= -93.3596\n",
      "differential_evolution step 17: f(x)= -93.3596\n",
      "differential_evolution step 18: f(x)= -93.3621\n",
      "differential_evolution step 19: f(x)= -93.4803\n",
      "differential_evolution step 20: f(x)= -93.5168\n",
      "I found hyperparameters  [0.78280255 7.23059506 7.0944846 ]  with likelihood  -93.5530056892724  via global optimization\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  100\n",
      "Run Time:  11.396478414535522      seconds\n",
      "Number of measurements:  100\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.03229603366955793\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.292685\n",
      "differential_evolution step 2: f(x)= -0.292685\n",
      "differential_evolution step 3: f(x)= -0.292685\n",
      "differential_evolution step 4: f(x)= -0.294851\n",
      "differential_evolution step 5: f(x)= -0.294851\n",
      "differential_evolution step 6: f(x)= -0.297579\n",
      "differential_evolution step 7: f(x)= -0.297579\n",
      "differential_evolution step 8: f(x)= -0.298427\n",
      "differential_evolution step 9: f(x)= -0.299217\n",
      "differential_evolution step 10: f(x)= -0.300074\n",
      "differential_evolution step 11: f(x)= -0.300074\n",
      "differential_evolution step 12: f(x)= -0.300115\n",
      "variance optimization tolerance of changed to:  0.030011485809258207\n",
      "Next points to be requested: \n",
      "[[1.01127103 9.04659433]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  101\n",
      "Run Time:  11.720509052276611      seconds\n",
      "Number of measurements:  101\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.030011485809258207\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0263652817687777\n",
      "Next points to be requested: \n",
      "[[1.76494202 3.10752974]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  102\n",
      "Run Time:  11.734225511550903      seconds\n",
      "Number of measurements:  102\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0263652817687777\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.292199\n",
      "differential_evolution step 2: f(x)= -0.30025\n",
      "differential_evolution step 3: f(x)= -0.303254\n",
      "differential_evolution step 4: f(x)= -0.303254\n",
      "differential_evolution step 5: f(x)= -0.303254\n",
      "differential_evolution step 6: f(x)= -0.303254\n",
      "differential_evolution step 7: f(x)= -0.303582\n",
      "differential_evolution step 8: f(x)= -0.303582\n",
      "differential_evolution step 9: f(x)= -0.303582\n",
      "differential_evolution step 10: f(x)= -0.303582\n",
      "differential_evolution step 11: f(x)= -0.303582\n",
      "differential_evolution step 12: f(x)= -0.304276\n",
      "differential_evolution step 13: f(x)= -0.304276\n",
      "differential_evolution step 14: f(x)= -0.304276\n",
      "differential_evolution step 15: f(x)= -0.304276\n",
      "differential_evolution step 16: f(x)= -0.304276\n",
      "differential_evolution step 17: f(x)= -0.305344\n",
      "differential_evolution step 18: f(x)= -0.305344\n",
      "variance optimization tolerance of changed to:  0.0305344245907513\n",
      "Next points to be requested: \n",
      "[[7.49575847 0.06116484]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  103\n",
      "Run Time:  12.178984880447388      seconds\n",
      "Number of measurements:  103\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.0305344245907513\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02593039667467284\n",
      "Next points to be requested: \n",
      "[[7.68685569 5.67075714]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  104\n",
      "Run Time:  12.194514036178589      seconds\n",
      "Number of measurements:  104\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02593039667467284\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.291058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 2: f(x)= -0.291058\n",
      "differential_evolution step 3: f(x)= -0.302576\n",
      "differential_evolution step 4: f(x)= -0.302576\n",
      "differential_evolution step 5: f(x)= -0.302576\n",
      "differential_evolution step 6: f(x)= -0.302603\n",
      "differential_evolution step 7: f(x)= -0.302603\n",
      "differential_evolution step 8: f(x)= -0.302603\n",
      "differential_evolution step 9: f(x)= -0.303333\n",
      "variance optimization tolerance of changed to:  0.030333340971110317\n",
      "Next points to be requested: \n",
      "[[8.96890131 1.00202417]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  105\n",
      "Run Time:  12.39944314956665      seconds\n",
      "Number of measurements:  105\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.030333340971110317\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024470190042968677\n",
      "Next points to be requested: \n",
      "[[5.34495564 0.48698714]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  106\n",
      "Run Time:  12.4131498336792      seconds\n",
      "Number of measurements:  106\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024470190042968677\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.278907\n",
      "differential_evolution step 2: f(x)= -0.286883\n",
      "differential_evolution step 3: f(x)= -0.288424\n",
      "differential_evolution step 4: f(x)= -0.288424\n",
      "differential_evolution step 5: f(x)= -0.294911\n",
      "differential_evolution step 6: f(x)= -0.294911\n",
      "differential_evolution step 7: f(x)= -0.294911\n",
      "differential_evolution step 8: f(x)= -0.294911\n",
      "differential_evolution step 9: f(x)= -0.294911\n",
      "differential_evolution step 10: f(x)= -0.296304\n",
      "differential_evolution step 11: f(x)= -0.296304\n",
      "differential_evolution step 12: f(x)= -0.296304\n",
      "differential_evolution step 13: f(x)= -0.296304\n",
      "differential_evolution step 14: f(x)= -0.296304\n",
      "differential_evolution step 15: f(x)= -0.296304\n",
      "differential_evolution step 16: f(x)= -0.296304\n",
      "variance optimization tolerance of changed to:  0.02963044238104065\n",
      "Next points to be requested: \n",
      "[[3.48026083 7.66197081]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  107\n",
      "Run Time:  12.858840703964233      seconds\n",
      "Number of measurements:  107\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02963044238104065\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02675891041217789\n",
      "Next points to be requested: \n",
      "[[1.98194688 2.42670598]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  108\n",
      "Run Time:  12.870265007019043      seconds\n",
      "Number of measurements:  108\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02675891041217789\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.286556\n",
      "differential_evolution step 2: f(x)= -0.286556\n",
      "differential_evolution step 3: f(x)= -0.286556\n",
      "differential_evolution step 4: f(x)= -0.286556\n",
      "differential_evolution step 5: f(x)= -0.286556\n",
      "differential_evolution step 6: f(x)= -0.286556\n",
      "differential_evolution step 7: f(x)= -0.289054\n",
      "differential_evolution step 8: f(x)= -0.289054\n",
      "variance optimization tolerance of changed to:  0.028905426714529822\n",
      "Next points to be requested: \n",
      "[[7.58335002 2.07403867]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  109\n",
      "Run Time:  13.059670209884644      seconds\n",
      "Number of measurements:  109\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028905426714529822\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02815357411017518\n",
      "Next points to be requested: \n",
      "[[8.31421324 9.35986916]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  110\n",
      "Run Time:  13.100842952728271      seconds\n",
      "Number of measurements:  110\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02815357411017518\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.284653\n",
      "differential_evolution step 2: f(x)= -0.287356\n",
      "differential_evolution step 3: f(x)= -0.287356\n",
      "differential_evolution step 4: f(x)= -0.287356\n",
      "differential_evolution step 5: f(x)= -0.287356\n",
      "differential_evolution step 6: f(x)= -0.287356\n",
      "differential_evolution step 7: f(x)= -0.290778\n",
      "differential_evolution step 8: f(x)= -0.290778\n",
      "differential_evolution step 9: f(x)= -0.290778\n",
      "differential_evolution step 10: f(x)= -0.290778\n",
      "differential_evolution step 11: f(x)= -0.290778\n",
      "differential_evolution step 12: f(x)= -0.290778\n",
      "differential_evolution step 13: f(x)= -0.290778\n",
      "differential_evolution step 14: f(x)= -0.29079\n",
      "variance optimization tolerance of changed to:  0.029079027948036507\n",
      "Next points to be requested: \n",
      "[[0.02128103 7.64736462]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  111\n",
      "Run Time:  13.467649459838867      seconds\n",
      "Number of measurements:  111\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.029079027948036507\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.025456142015806156\n",
      "Next points to be requested: \n",
      "[[0.79711049 1.72688009]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  112\n",
      "Run Time:  13.48257040977478      seconds\n",
      "Number of measurements:  112\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.025456142015806156\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.286298\n",
      "differential_evolution step 2: f(x)= -0.286298\n",
      "differential_evolution step 3: f(x)= -0.286298\n",
      "differential_evolution step 4: f(x)= -0.286298\n",
      "differential_evolution step 5: f(x)= -0.286298\n",
      "differential_evolution step 6: f(x)= -0.286603\n",
      "differential_evolution step 7: f(x)= -0.286603\n",
      "differential_evolution step 8: f(x)= -0.286603\n",
      "differential_evolution step 9: f(x)= -0.286603\n",
      "differential_evolution step 10: f(x)= -0.286603\n",
      "differential_evolution step 11: f(x)= -0.286603\n",
      "variance optimization tolerance of changed to:  0.02866025519178163\n",
      "Next points to be requested: \n",
      "[[3.47283078 6.19484246]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  113\n",
      "Run Time:  13.794562578201294      seconds\n",
      "Number of measurements:  113\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02866025519178163\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02311344532927024\n",
      "Next points to be requested: \n",
      "[[7.65000989 2.54050364]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  114\n",
      "Run Time:  13.809672832489014      seconds\n",
      "Number of measurements:  114\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02311344532927024\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.276233\n",
      "differential_evolution step 2: f(x)= -0.283795\n",
      "differential_evolution step 3: f(x)= -0.283795\n",
      "differential_evolution step 4: f(x)= -0.284191\n",
      "differential_evolution step 5: f(x)= -0.287912\n",
      "differential_evolution step 6: f(x)= -0.287912\n",
      "differential_evolution step 7: f(x)= -0.287912\n",
      "differential_evolution step 8: f(x)= -0.287912\n",
      "differential_evolution step 9: f(x)= -0.287994\n",
      "differential_evolution step 10: f(x)= -0.287994\n",
      "differential_evolution step 11: f(x)= -0.287994\n",
      "differential_evolution step 12: f(x)= -0.288\n",
      "variance optimization tolerance of changed to:  0.028800006011255454\n",
      "Next points to be requested: \n",
      "[[4.9357329  1.80188684]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  115\n",
      "Run Time:  14.169954776763916      seconds\n",
      "Number of measurements:  115\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028800006011255454\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.025383693102809402\n",
      "Next points to be requested: \n",
      "[[9.85579953 5.55112844]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  116\n",
      "Run Time:  14.185073614120483      seconds\n",
      "Number of measurements:  116\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.025383693102809402\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.289398\n",
      "differential_evolution step 2: f(x)= -0.289398\n",
      "differential_evolution step 3: f(x)= -0.289398\n",
      "differential_evolution step 4: f(x)= -0.289398\n",
      "differential_evolution step 5: f(x)= -0.289398\n",
      "differential_evolution step 6: f(x)= -0.289398\n",
      "differential_evolution step 7: f(x)= -0.289398\n",
      "differential_evolution step 8: f(x)= -0.289548\n",
      "differential_evolution step 9: f(x)= -0.289548\n",
      "differential_evolution step 10: f(x)= -0.289548\n",
      "differential_evolution step 11: f(x)= -0.289548\n",
      "differential_evolution step 12: f(x)= -0.289548\n",
      "differential_evolution step 13: f(x)= -0.289548\n",
      "differential_evolution step 14: f(x)= -0.289548\n",
      "differential_evolution step 15: f(x)= -0.289548\n",
      "variance optimization tolerance of changed to:  0.02895480576565966\n",
      "Next points to be requested: \n",
      "[[6.84629878 3.63965934]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  117\n",
      "Run Time:  14.548817873001099      seconds\n",
      "Number of measurements:  117\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02895480576565966\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023536496535267185\n",
      "Next points to be requested: \n",
      "[[6.53213912 4.06558908]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  118\n",
      "Run Time:  14.56261920928955      seconds\n",
      "Number of measurements:  118\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023536496535267185\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.274624\n",
      "differential_evolution step 2: f(x)= -0.274624\n",
      "differential_evolution step 3: f(x)= -0.278588\n",
      "differential_evolution step 4: f(x)= -0.2822\n",
      "differential_evolution step 5: f(x)= -0.2822\n",
      "differential_evolution step 6: f(x)= -0.2822\n",
      "differential_evolution step 7: f(x)= -0.284197\n",
      "differential_evolution step 8: f(x)= -0.284197\n",
      "differential_evolution step 9: f(x)= -0.284197\n",
      "differential_evolution step 10: f(x)= -0.284197\n",
      "differential_evolution step 11: f(x)= -0.284197\n",
      "differential_evolution step 12: f(x)= -0.285027\n",
      "differential_evolution step 13: f(x)= -0.285027\n",
      "differential_evolution step 14: f(x)= -0.285027\n",
      "differential_evolution step 15: f(x)= -0.285437\n",
      "differential_evolution step 16: f(x)= -0.285437\n",
      "differential_evolution step 17: f(x)= -0.285437\n",
      "differential_evolution step 18: f(x)= -0.285465\n",
      "variance optimization tolerance of changed to:  0.028546451985944323\n",
      "Next points to be requested: \n",
      "[[3.62007067 8.95659532]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  119\n",
      "Run Time:  15.046253442764282      seconds\n",
      "Number of measurements:  119\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028546451985944323\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021496123646947096\n",
      "Next points to be requested: \n",
      "[[7.40719996 8.64876876]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  120\n",
      "Run Time:  15.05998706817627      seconds\n",
      "Number of measurements:  120\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021496123646947096\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.276694\n",
      "differential_evolution step 2: f(x)= -0.276694\n",
      "differential_evolution step 3: f(x)= -0.278867\n",
      "differential_evolution step 4: f(x)= -0.278976\n",
      "differential_evolution step 5: f(x)= -0.280387\n",
      "differential_evolution step 6: f(x)= -0.280387\n",
      "differential_evolution step 7: f(x)= -0.283582\n",
      "differential_evolution step 8: f(x)= -0.283582\n",
      "differential_evolution step 9: f(x)= -0.283582\n",
      "differential_evolution step 10: f(x)= -0.283582\n",
      "differential_evolution step 11: f(x)= -0.283582\n",
      "differential_evolution step 12: f(x)= -0.283582\n",
      "differential_evolution step 13: f(x)= -0.283582\n",
      "differential_evolution step 14: f(x)= -0.283582\n",
      "differential_evolution step 15: f(x)= -0.283582\n",
      "differential_evolution step 16: f(x)= -0.283892\n",
      "differential_evolution step 17: f(x)= -0.283892\n",
      "differential_evolution step 18: f(x)= -0.283892\n",
      "differential_evolution step 19: f(x)= -0.283892\n",
      "differential_evolution step 20: f(x)= -0.283999\n",
      "variance optimization tolerance of changed to:  0.028399893599920147\n",
      "Next points to be requested: \n",
      "[[7.80592756 4.28621495]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  121\n",
      "Run Time:  15.572822570800781      seconds\n",
      "Number of measurements:  121\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028399893599920147\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021441171238221705\n",
      "Next points to be requested: \n",
      "[[6.39269224 4.54146312]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  122\n",
      "Run Time:  15.589029788970947      seconds\n",
      "Number of measurements:  122\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021441171238221705\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.280088\n",
      "differential_evolution step 2: f(x)= -0.280211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 3: f(x)= -0.280211\n",
      "differential_evolution step 4: f(x)= -0.280211\n",
      "differential_evolution step 5: f(x)= -0.280211\n",
      "differential_evolution step 6: f(x)= -0.280211\n",
      "differential_evolution step 7: f(x)= -0.280211\n",
      "differential_evolution step 8: f(x)= -0.285111\n",
      "differential_evolution step 9: f(x)= -0.285111\n",
      "differential_evolution step 10: f(x)= -0.285111\n",
      "differential_evolution step 11: f(x)= -0.285111\n",
      "differential_evolution step 12: f(x)= -0.285111\n",
      "differential_evolution step 13: f(x)= -0.285351\n",
      "differential_evolution step 14: f(x)= -0.285351\n",
      "differential_evolution step 15: f(x)= -0.285351\n",
      "variance optimization tolerance of changed to:  0.028535078625152494\n",
      "Next points to be requested: \n",
      "[[2.34340622 7.54521609]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  123\n",
      "Run Time:  16.040400505065918      seconds\n",
      "Number of measurements:  123\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028535078625152494\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02498622909422998\n",
      "Next points to be requested: \n",
      "[[2.53825196 3.41892371]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  124\n",
      "Run Time:  16.05889058113098      seconds\n",
      "Number of measurements:  124\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02498622909422998\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.275231\n",
      "differential_evolution step 2: f(x)= -0.275231\n",
      "differential_evolution step 3: f(x)= -0.282787\n",
      "differential_evolution step 4: f(x)= -0.282787\n",
      "differential_evolution step 5: f(x)= -0.282787\n",
      "differential_evolution step 6: f(x)= -0.282787\n",
      "differential_evolution step 7: f(x)= -0.282787\n",
      "differential_evolution step 8: f(x)= -0.282787\n",
      "differential_evolution step 9: f(x)= -0.282787\n",
      "differential_evolution step 10: f(x)= -0.282787\n",
      "differential_evolution step 11: f(x)= -0.282787\n",
      "differential_evolution step 12: f(x)= -0.282787\n",
      "differential_evolution step 13: f(x)= -0.282787\n",
      "differential_evolution step 14: f(x)= -0.282787\n",
      "differential_evolution step 15: f(x)= -0.282787\n",
      "differential_evolution step 16: f(x)= -0.282787\n",
      "differential_evolution step 17: f(x)= -0.282787\n",
      "differential_evolution step 18: f(x)= -0.282787\n",
      "differential_evolution step 19: f(x)= -0.282787\n",
      "differential_evolution step 20: f(x)= -0.282787\n",
      "variance optimization tolerance of changed to:  0.028278728467951243\n",
      "Next points to be requested: \n",
      "[[0.06014054 9.26283441]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  125\n",
      "Run Time:  16.64527940750122      seconds\n",
      "Number of measurements:  125\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.028278728467951243\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02555403249648306\n",
      "Next points to be requested: \n",
      "[[6.90791234 7.86884529]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  126\n",
      "Run Time:  16.672569274902344      seconds\n",
      "Number of measurements:  126\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02555403249648306\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.277973\n",
      "differential_evolution step 2: f(x)= -0.279259\n",
      "differential_evolution step 3: f(x)= -0.279259\n",
      "differential_evolution step 4: f(x)= -0.280518\n",
      "differential_evolution step 5: f(x)= -0.285701\n",
      "differential_evolution step 6: f(x)= -0.285701\n",
      "differential_evolution step 7: f(x)= -0.285701\n",
      "differential_evolution step 8: f(x)= -0.290622\n",
      "differential_evolution step 9: f(x)= -0.290622\n",
      "differential_evolution step 10: f(x)= -0.291262\n",
      "differential_evolution step 11: f(x)= -0.291262\n",
      "differential_evolution step 12: f(x)= -0.291262\n",
      "differential_evolution step 13: f(x)= -0.291262\n",
      "differential_evolution step 14: f(x)= -0.291262\n",
      "differential_evolution step 15: f(x)= -0.291808\n",
      "differential_evolution step 16: f(x)= -0.297879\n",
      "differential_evolution step 17: f(x)= -0.297879\n",
      "differential_evolution step 18: f(x)= -0.29848\n",
      "differential_evolution step 19: f(x)= -0.300715\n",
      "differential_evolution step 20: f(x)= -0.301057\n",
      "variance optimization tolerance of changed to:  0.030105736559794843\n",
      "Next points to be requested: \n",
      "[[9.99681353 8.01594124]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  127\n",
      "Run Time:  17.31103277206421      seconds\n",
      "Number of measurements:  127\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.030105736559794843\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02178065802664301\n",
      "Next points to be requested: \n",
      "[[0.33152724 8.06691429]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  128\n",
      "Run Time:  17.3239643573761      seconds\n",
      "Number of measurements:  128\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02178065802664301\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.276743\n",
      "differential_evolution step 2: f(x)= -0.276743\n",
      "differential_evolution step 3: f(x)= -0.276743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 4: f(x)= -0.276743\n",
      "differential_evolution step 5: f(x)= -0.276743\n",
      "differential_evolution step 6: f(x)= -0.277601\n",
      "differential_evolution step 7: f(x)= -0.280027\n",
      "differential_evolution step 8: f(x)= -0.280027\n",
      "differential_evolution step 9: f(x)= -0.28162\n",
      "differential_evolution step 10: f(x)= -0.28162\n",
      "differential_evolution step 11: f(x)= -0.281902\n",
      "variance optimization tolerance of changed to:  0.02819020007576664\n",
      "Next points to be requested: \n",
      "[[0.85239481 4.86489001]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  129\n",
      "Run Time:  17.622419357299805      seconds\n",
      "Number of measurements:  129\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02819020007576664\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.0243445280205074\n",
      "Next points to be requested: \n",
      "[[2.86736231 9.01620987]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  130\n",
      "Run Time:  17.639484882354736      seconds\n",
      "Number of measurements:  130\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.0243445280205074\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.271437\n",
      "differential_evolution step 2: f(x)= -0.274605\n",
      "differential_evolution step 3: f(x)= -0.274605\n",
      "differential_evolution step 4: f(x)= -0.274605\n",
      "differential_evolution step 5: f(x)= -0.274605\n",
      "differential_evolution step 6: f(x)= -0.277436\n",
      "differential_evolution step 7: f(x)= -0.277436\n",
      "differential_evolution step 8: f(x)= -0.277436\n",
      "differential_evolution step 9: f(x)= -0.277436\n",
      "differential_evolution step 10: f(x)= -0.277649\n",
      "differential_evolution step 11: f(x)= -0.277649\n",
      "differential_evolution step 12: f(x)= -0.277649\n",
      "differential_evolution step 13: f(x)= -0.277649\n",
      "differential_evolution step 14: f(x)= -0.277649\n",
      "differential_evolution step 15: f(x)= -0.280518\n",
      "differential_evolution step 16: f(x)= -0.281532\n",
      "differential_evolution step 17: f(x)= -0.281532\n",
      "differential_evolution step 18: f(x)= -0.282783\n",
      "differential_evolution step 19: f(x)= -0.282783\n",
      "differential_evolution step 20: f(x)= -0.282783\n",
      "variance optimization tolerance of changed to:  0.02827834931493481\n",
      "Next points to be requested: \n",
      "[[1.9720105  1.13024661]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  131\n",
      "Run Time:  18.273971796035767      seconds\n",
      "Number of measurements:  131\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02827834931493481\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024535819337510964\n",
      "Next points to be requested: \n",
      "[[3.90543925 8.51915341]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  132\n",
      "Run Time:  18.289944410324097      seconds\n",
      "Number of measurements:  132\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024535819337510964\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.272434\n",
      "differential_evolution step 2: f(x)= -0.272895\n",
      "differential_evolution step 3: f(x)= -0.272895\n",
      "differential_evolution step 4: f(x)= -0.273305\n",
      "differential_evolution step 5: f(x)= -0.273305\n",
      "differential_evolution step 6: f(x)= -0.273305\n",
      "differential_evolution step 7: f(x)= -0.273305\n",
      "differential_evolution step 8: f(x)= -0.275285\n",
      "differential_evolution step 9: f(x)= -0.275285\n",
      "differential_evolution step 10: f(x)= -0.275285\n",
      "differential_evolution step 11: f(x)= -0.278023\n",
      "differential_evolution step 12: f(x)= -0.278023\n",
      "differential_evolution step 13: f(x)= -0.278023\n",
      "differential_evolution step 14: f(x)= -0.278023\n",
      "differential_evolution step 15: f(x)= -0.278023\n",
      "differential_evolution step 16: f(x)= -0.278023\n",
      "differential_evolution step 17: f(x)= -0.278023\n",
      "differential_evolution step 18: f(x)= -0.278023\n",
      "differential_evolution step 19: f(x)= -0.278023\n",
      "variance optimization tolerance of changed to:  0.02780234145300907\n",
      "Next points to be requested: \n",
      "[[9.04360004 2.94998481]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  133\n",
      "Run Time:  18.782243490219116      seconds\n",
      "Number of measurements:  133\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02780234145300907\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02222493306020562\n",
      "Next points to be requested: \n",
      "[[9.48228326 5.30278212]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  134\n",
      "Run Time:  18.83547568321228      seconds\n",
      "Number of measurements:  134\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02222493306020562\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.272211\n",
      "differential_evolution step 2: f(x)= -0.272211\n",
      "differential_evolution step 3: f(x)= -0.280487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 4: f(x)= -0.280487\n",
      "differential_evolution step 5: f(x)= -0.280487\n",
      "differential_evolution step 6: f(x)= -0.280487\n",
      "differential_evolution step 7: f(x)= -0.280487\n",
      "differential_evolution step 8: f(x)= -0.280487\n",
      "differential_evolution step 9: f(x)= -0.280487\n",
      "differential_evolution step 10: f(x)= -0.280487\n",
      "differential_evolution step 11: f(x)= -0.280487\n",
      "differential_evolution step 12: f(x)= -0.280487\n",
      "differential_evolution step 13: f(x)= -0.280487\n",
      "differential_evolution step 14: f(x)= -0.281573\n",
      "differential_evolution step 15: f(x)= -0.281573\n",
      "differential_evolution step 16: f(x)= -0.281573\n",
      "differential_evolution step 17: f(x)= -0.281812\n",
      "differential_evolution step 18: f(x)= -0.281812\n",
      "differential_evolution step 19: f(x)= -0.281812\n",
      "differential_evolution step 20: f(x)= -0.281812\n",
      "variance optimization tolerance of changed to:  0.02818115565395923\n",
      "Next points to be requested: \n",
      "[[6.00990752 1.81650483]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  135\n",
      "Run Time:  19.45741605758667      seconds\n",
      "Number of measurements:  135\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02818115565395923\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024412088719510915\n",
      "Next points to be requested: \n",
      "[[1.52741646 8.7478415 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  136\n",
      "Run Time:  19.47403073310852      seconds\n",
      "Number of measurements:  136\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024412088719510915\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.270633\n",
      "differential_evolution step 2: f(x)= -0.270633\n",
      "differential_evolution step 3: f(x)= -0.270633\n",
      "differential_evolution step 4: f(x)= -0.270633\n",
      "differential_evolution step 5: f(x)= -0.270633\n",
      "differential_evolution step 6: f(x)= -0.272502\n",
      "differential_evolution step 7: f(x)= -0.275234\n",
      "differential_evolution step 8: f(x)= -0.278336\n",
      "differential_evolution step 9: f(x)= -0.278336\n",
      "differential_evolution step 10: f(x)= -0.278336\n",
      "differential_evolution step 11: f(x)= -0.278387\n",
      "differential_evolution step 12: f(x)= -0.278417\n",
      "differential_evolution step 13: f(x)= -0.278417\n",
      "differential_evolution step 14: f(x)= -0.278417\n",
      "differential_evolution step 15: f(x)= -0.278417\n",
      "differential_evolution step 16: f(x)= -0.278417\n",
      "variance optimization tolerance of changed to:  0.027841666423328144\n",
      "Next points to be requested: \n",
      "[[0.81886108 6.53909331]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  137\n",
      "Run Time:  20.014380931854248      seconds\n",
      "Number of measurements:  137\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.027841666423328144\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023766851993966973\n",
      "Next points to be requested: \n",
      "[[2.96426449 4.77388071]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  138\n",
      "Run Time:  20.028376817703247      seconds\n",
      "Number of measurements:  138\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023766851993966973\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.271199\n",
      "differential_evolution step 2: f(x)= -0.271199\n",
      "differential_evolution step 3: f(x)= -0.271199\n",
      "differential_evolution step 4: f(x)= -0.275371\n",
      "differential_evolution step 5: f(x)= -0.275371\n",
      "differential_evolution step 6: f(x)= -0.275371\n",
      "differential_evolution step 7: f(x)= -0.275371\n",
      "differential_evolution step 8: f(x)= -0.275371\n",
      "differential_evolution step 9: f(x)= -0.275371\n",
      "differential_evolution step 10: f(x)= -0.275371\n",
      "differential_evolution step 11: f(x)= -0.275371\n",
      "differential_evolution step 12: f(x)= -0.275371\n",
      "differential_evolution step 13: f(x)= -0.275371\n",
      "differential_evolution step 14: f(x)= -0.275645\n",
      "differential_evolution step 15: f(x)= -0.275902\n",
      "variance optimization tolerance of changed to:  0.02759021492246442\n",
      "Next points to be requested: \n",
      "[[5.23905305 9.14049718]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  139\n",
      "Run Time:  20.50813317298889      seconds\n",
      "Number of measurements:  139\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02759021492246442\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance optimization tolerance of changed to:  0.02211950133943317\n",
      "Next points to be requested: \n",
      "[[9.57587754 4.87969132]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  140\n",
      "Run Time:  20.523440837860107      seconds\n",
      "Number of measurements:  140\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02211950133943317\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.264778\n",
      "differential_evolution step 2: f(x)= -0.280733\n",
      "differential_evolution step 3: f(x)= -0.280733\n",
      "differential_evolution step 4: f(x)= -0.280733\n",
      "differential_evolution step 5: f(x)= -0.280733\n",
      "differential_evolution step 6: f(x)= -0.280733\n",
      "differential_evolution step 7: f(x)= -0.280733\n",
      "differential_evolution step 8: f(x)= -0.280735\n",
      "differential_evolution step 9: f(x)= -0.280735\n",
      "differential_evolution step 10: f(x)= -0.280735\n",
      "differential_evolution step 11: f(x)= -0.28562\n",
      "differential_evolution step 12: f(x)= -0.28562\n",
      "differential_evolution step 13: f(x)= -0.28562\n",
      "differential_evolution step 14: f(x)= -0.28562\n",
      "differential_evolution step 15: f(x)= -0.28562\n",
      "differential_evolution step 16: f(x)= -0.28562\n",
      "differential_evolution step 17: f(x)= -0.28562\n",
      "differential_evolution step 18: f(x)= -0.28562\n",
      "differential_evolution step 19: f(x)= -0.28562\n",
      "differential_evolution step 20: f(x)= -0.28562\n",
      "variance optimization tolerance of changed to:  0.02856201677039945\n",
      "Next points to be requested: \n",
      "[[9.99853507 6.82082812]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  141\n",
      "Run Time:  21.1209397315979      seconds\n",
      "Number of measurements:  141\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02856201677039945\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021363732916784738\n",
      "Next points to be requested: \n",
      "[[10.          5.18219025]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  142\n",
      "Run Time:  21.135457515716553      seconds\n",
      "Number of measurements:  142\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021363732916784738\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.264948\n",
      "differential_evolution step 2: f(x)= -0.268754\n",
      "differential_evolution step 3: f(x)= -0.269016\n",
      "differential_evolution step 4: f(x)= -0.269297\n",
      "differential_evolution step 5: f(x)= -0.269297\n",
      "differential_evolution step 6: f(x)= -0.269561\n",
      "differential_evolution step 7: f(x)= -0.270217\n",
      "differential_evolution step 8: f(x)= -0.272279\n",
      "differential_evolution step 9: f(x)= -0.272279\n",
      "differential_evolution step 10: f(x)= -0.272279\n",
      "differential_evolution step 11: f(x)= -0.272285\n",
      "differential_evolution step 12: f(x)= -0.272285\n",
      "variance optimization tolerance of changed to:  0.02722854440555242\n",
      "Next points to be requested: \n",
      "[[2.22766351 5.47433951]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  143\n",
      "Run Time:  21.565858125686646      seconds\n",
      "Number of measurements:  143\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02722854440555242\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02366113588149986\n",
      "Next points to be requested: \n",
      "[[1.21195046 7.57563425]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  144\n",
      "Run Time:  21.584357261657715      seconds\n",
      "Number of measurements:  144\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02366113588149986\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.266625\n",
      "differential_evolution step 2: f(x)= -0.266625\n",
      "differential_evolution step 3: f(x)= -0.266625\n",
      "differential_evolution step 4: f(x)= -0.266625\n",
      "differential_evolution step 5: f(x)= -0.266962\n",
      "differential_evolution step 6: f(x)= -0.266962\n",
      "differential_evolution step 7: f(x)= -0.268017\n",
      "differential_evolution step 8: f(x)= -0.268017\n",
      "differential_evolution step 9: f(x)= -0.268209\n",
      "differential_evolution step 10: f(x)= -0.268209\n",
      "differential_evolution step 11: f(x)= -0.268209\n",
      "differential_evolution step 12: f(x)= -0.268273\n",
      "variance optimization tolerance of changed to:  0.02682731947275481\n",
      "Next points to be requested: \n",
      "[[9.5242004  1.93834059]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  145\n",
      "Run Time:  22.001777172088623      seconds\n",
      "Number of measurements:  145\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02682731947275481\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023464794753067286\n",
      "Next points to be requested: \n",
      "[[1.42693928 6.5481083 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  146\n",
      "Run Time:  22.015929222106934      seconds\n",
      "Number of measurements:  146\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023464794753067286\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.258936\n",
      "differential_evolution step 2: f(x)= -0.265527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 3: f(x)= -0.265912\n",
      "differential_evolution step 4: f(x)= -0.265912\n",
      "differential_evolution step 5: f(x)= -0.265912\n",
      "differential_evolution step 6: f(x)= -0.265912\n",
      "differential_evolution step 7: f(x)= -0.265912\n",
      "differential_evolution step 8: f(x)= -0.267941\n",
      "differential_evolution step 9: f(x)= -0.268732\n",
      "differential_evolution step 10: f(x)= -0.268834\n",
      "differential_evolution step 11: f(x)= -0.268834\n",
      "differential_evolution step 12: f(x)= -0.268856\n",
      "variance optimization tolerance of changed to:  0.026885637679458226\n",
      "Next points to be requested: \n",
      "[[5.26379208 6.10881196]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  147\n",
      "Run Time:  22.432472705841064      seconds\n",
      "Number of measurements:  147\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026885637679458226\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.025350151348342356\n",
      "Next points to be requested: \n",
      "[[2.2546756  4.54135121]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  148\n",
      "Run Time:  22.44550323486328      seconds\n",
      "Number of measurements:  148\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.025350151348342356\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.261177\n",
      "differential_evolution step 2: f(x)= -0.261177\n",
      "differential_evolution step 3: f(x)= -0.261177\n",
      "differential_evolution step 4: f(x)= -0.263003\n",
      "differential_evolution step 5: f(x)= -0.274146\n",
      "differential_evolution step 6: f(x)= -0.274146\n",
      "differential_evolution step 7: f(x)= -0.274146\n",
      "differential_evolution step 8: f(x)= -0.275261\n",
      "differential_evolution step 9: f(x)= -0.275261\n",
      "differential_evolution step 10: f(x)= -0.275261\n",
      "differential_evolution step 11: f(x)= -0.275261\n",
      "differential_evolution step 12: f(x)= -0.275261\n",
      "differential_evolution step 13: f(x)= -0.27831\n",
      "differential_evolution step 14: f(x)= -0.27831\n",
      "differential_evolution step 15: f(x)= -0.27831\n",
      "differential_evolution step 16: f(x)= -0.27831\n",
      "differential_evolution step 17: f(x)= -0.27831\n",
      "differential_evolution step 18: f(x)= -0.27831\n",
      "differential_evolution step 19: f(x)= -0.27831\n",
      "differential_evolution step 20: f(x)= -0.27831\n",
      "variance optimization tolerance of changed to:  0.027830981567415997\n",
      "Next points to be requested: \n",
      "[[9.26772587 0.0342961 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  149\n",
      "Run Time:  23.101515531539917      seconds\n",
      "Number of measurements:  149\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.027830981567415997\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.027109965715210772\n",
      "Next points to be requested: \n",
      "[[3.75611638 3.556916  ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  150\n",
      "Run Time:  23.12797260284424      seconds\n",
      "Number of measurements:  150\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.027109965715210772\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.259323\n",
      "differential_evolution step 2: f(x)= -0.261191\n",
      "differential_evolution step 3: f(x)= -0.261191\n",
      "differential_evolution step 4: f(x)= -0.261191\n",
      "differential_evolution step 5: f(x)= -0.261191\n",
      "differential_evolution step 6: f(x)= -0.26122\n",
      "differential_evolution step 7: f(x)= -0.26122\n",
      "differential_evolution step 8: f(x)= -0.26122\n",
      "differential_evolution step 9: f(x)= -0.26122\n",
      "differential_evolution step 10: f(x)= -0.263513\n",
      "differential_evolution step 11: f(x)= -0.263513\n",
      "differential_evolution step 12: f(x)= -0.263972\n",
      "differential_evolution step 13: f(x)= -0.264392\n",
      "differential_evolution step 14: f(x)= -0.264392\n",
      "variance optimization tolerance of changed to:  0.026439169473462754\n",
      "Next points to be requested: \n",
      "[[3.58608946 1.84657654]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  151\n",
      "Run Time:  23.637211084365845      seconds\n",
      "Number of measurements:  151\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026439169473462754\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024885604314572114\n",
      "Next points to be requested: \n",
      "[[6.34339619 7.05524455]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  152\n",
      "Run Time:  23.655534744262695      seconds\n",
      "Number of measurements:  152\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024885604314572114\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.2577\n",
      "differential_evolution step 2: f(x)= -0.258317\n",
      "differential_evolution step 3: f(x)= -0.259433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 4: f(x)= -0.259433\n",
      "differential_evolution step 5: f(x)= -0.259433\n",
      "differential_evolution step 6: f(x)= -0.259998\n",
      "differential_evolution step 7: f(x)= -0.261554\n",
      "differential_evolution step 8: f(x)= -0.262254\n",
      "differential_evolution step 9: f(x)= -0.263288\n",
      "differential_evolution step 10: f(x)= -0.263288\n",
      "differential_evolution step 11: f(x)= -0.263357\n",
      "differential_evolution step 12: f(x)= -0.263445\n",
      "differential_evolution step 13: f(x)= -0.263445\n",
      "differential_evolution step 14: f(x)= -0.263445\n",
      "differential_evolution step 15: f(x)= -0.263445\n",
      "variance optimization tolerance of changed to:  0.026344543136973316\n",
      "Next points to be requested: \n",
      "[[5.32415612 4.98921042]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  153\n",
      "Run Time:  24.186018705368042      seconds\n",
      "Number of measurements:  153\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026344543136973316\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023345734098369292\n",
      "Next points to be requested: \n",
      "[[6.22791332 9.4958447 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  154\n",
      "Run Time:  24.20326566696167      seconds\n",
      "Number of measurements:  154\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023345734098369292\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.254394\n",
      "differential_evolution step 2: f(x)= -0.260306\n",
      "differential_evolution step 3: f(x)= -0.260306\n",
      "differential_evolution step 4: f(x)= -0.260306\n",
      "differential_evolution step 5: f(x)= -0.260306\n",
      "differential_evolution step 6: f(x)= -0.260306\n",
      "differential_evolution step 7: f(x)= -0.260306\n",
      "differential_evolution step 8: f(x)= -0.260306\n",
      "differential_evolution step 9: f(x)= -0.260306\n",
      "differential_evolution step 10: f(x)= -0.260306\n",
      "differential_evolution step 11: f(x)= -0.260306\n",
      "differential_evolution step 12: f(x)= -0.260306\n",
      "differential_evolution step 13: f(x)= -0.260306\n",
      "differential_evolution step 14: f(x)= -0.263346\n",
      "differential_evolution step 15: f(x)= -0.269589\n",
      "differential_evolution step 16: f(x)= -0.269589\n",
      "differential_evolution step 17: f(x)= -0.274578\n",
      "differential_evolution step 18: f(x)= -0.274578\n",
      "differential_evolution step 19: f(x)= -0.274578\n",
      "differential_evolution step 20: f(x)= -0.274578\n",
      "variance optimization tolerance of changed to:  0.027457839493977794\n",
      "Next points to be requested: \n",
      "[[0.03646077 2.13420655]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  155\n",
      "Run Time:  24.914282083511353      seconds\n",
      "Number of measurements:  155\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.027457839493977794\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023557325979800495\n",
      "Next points to be requested: \n",
      "[[7.31441847 4.01852637]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  156\n",
      "Run Time:  24.93256640434265      seconds\n",
      "Number of measurements:  156\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023557325979800495\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.261867\n",
      "differential_evolution step 2: f(x)= -0.261867\n",
      "differential_evolution step 3: f(x)= -0.261867\n",
      "differential_evolution step 4: f(x)= -0.261867\n",
      "differential_evolution step 5: f(x)= -0.261867\n",
      "differential_evolution step 6: f(x)= -0.261867\n",
      "differential_evolution step 7: f(x)= -0.262136\n",
      "differential_evolution step 8: f(x)= -0.262136\n",
      "differential_evolution step 9: f(x)= -0.262136\n",
      "differential_evolution step 10: f(x)= -0.262136\n",
      "differential_evolution step 11: f(x)= -0.262136\n",
      "differential_evolution step 12: f(x)= -0.262136\n",
      "differential_evolution step 13: f(x)= -0.262136\n",
      "differential_evolution step 14: f(x)= -0.262136\n",
      "differential_evolution step 15: f(x)= -0.262136\n",
      "differential_evolution step 16: f(x)= -0.262285\n",
      "differential_evolution step 17: f(x)= -0.262285\n",
      "differential_evolution step 18: f(x)= -0.262309\n",
      "differential_evolution step 19: f(x)= -0.262309\n",
      "variance optimization tolerance of changed to:  0.026230881163590326\n",
      "Next points to be requested: \n",
      "[[4.60365166 0.62872261]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  157\n",
      "Run Time:  25.667271852493286      seconds\n",
      "Number of measurements:  157\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026230881163590326\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024555603108205842\n",
      "Next points to be requested: \n",
      "[[8.13316509 0.42156698]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  158\n",
      "Run Time:  25.68791699409485      seconds\n",
      "Number of measurements:  158\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024555603108205842\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.25439\n",
      "differential_evolution step 2: f(x)= -0.25439\n",
      "differential_evolution step 3: f(x)= -0.25439\n",
      "differential_evolution step 4: f(x)= -0.260731\n",
      "differential_evolution step 5: f(x)= -0.263019\n",
      "differential_evolution step 6: f(x)= -0.263019\n",
      "differential_evolution step 7: f(x)= -0.263019\n",
      "differential_evolution step 8: f(x)= -0.263019\n",
      "differential_evolution step 9: f(x)= -0.263019\n",
      "differential_evolution step 10: f(x)= -0.263019\n",
      "differential_evolution step 11: f(x)= -0.263019\n",
      "differential_evolution step 12: f(x)= -0.263019\n",
      "differential_evolution step 13: f(x)= -0.263843\n",
      "differential_evolution step 14: f(x)= -0.263843\n",
      "differential_evolution step 15: f(x)= -0.266072\n",
      "differential_evolution step 16: f(x)= -0.266072\n",
      "differential_evolution step 17: f(x)= -0.266223\n",
      "differential_evolution step 18: f(x)= -0.266223\n",
      "differential_evolution step 19: f(x)= -0.266223\n",
      "differential_evolution step 20: f(x)= -0.266223\n",
      "variance optimization tolerance of changed to:  0.02662234519266342\n",
      "Next points to be requested: \n",
      "[[9.97069452 0.63680804]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  159\n",
      "Run Time:  26.400635480880737      seconds\n",
      "Number of measurements:  159\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02662234519266342\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023021359784840014\n",
      "Next points to be requested: \n",
      "[[4.00870496 5.80007748]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  160\n",
      "Run Time:  26.421185970306396      seconds\n",
      "Number of measurements:  160\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023021359784840014\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.254576\n",
      "differential_evolution step 2: f(x)= -0.254576\n",
      "differential_evolution step 3: f(x)= -0.259566\n",
      "differential_evolution step 4: f(x)= -0.259566\n",
      "differential_evolution step 5: f(x)= -0.259566\n",
      "differential_evolution step 6: f(x)= -0.259566\n",
      "differential_evolution step 7: f(x)= -0.259566\n",
      "differential_evolution step 8: f(x)= -0.259566\n",
      "differential_evolution step 9: f(x)= -0.259566\n",
      "differential_evolution step 10: f(x)= -0.259566\n",
      "differential_evolution step 11: f(x)= -0.259566\n",
      "differential_evolution step 12: f(x)= -0.259566\n",
      "differential_evolution step 13: f(x)= -0.259566\n",
      "differential_evolution step 14: f(x)= -0.260216\n",
      "differential_evolution step 15: f(x)= -0.260386\n",
      "differential_evolution step 16: f(x)= -0.260386\n",
      "differential_evolution step 17: f(x)= -0.260386\n",
      "differential_evolution step 18: f(x)= -0.260386\n",
      "differential_evolution step 19: f(x)= -0.260386\n",
      "variance optimization tolerance of changed to:  0.026038590594049893\n",
      "Next points to be requested: \n",
      "[[3.78348331 6.89934795]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  161\n",
      "Run Time:  27.052927017211914      seconds\n",
      "Number of measurements:  161\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026038590594049893\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021956814305971212\n",
      "Next points to be requested: \n",
      "[[7.05779266 0.        ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  162\n",
      "Run Time:  27.122007369995117      seconds\n",
      "Number of measurements:  162\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021956814305971212\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.253272\n",
      "differential_evolution step 2: f(x)= -0.253272\n",
      "differential_evolution step 3: f(x)= -0.253272\n",
      "differential_evolution step 4: f(x)= -0.253272\n",
      "differential_evolution step 5: f(x)= -0.259005\n",
      "differential_evolution step 6: f(x)= -0.259005\n",
      "differential_evolution step 7: f(x)= -0.259005\n",
      "differential_evolution step 8: f(x)= -0.259005\n",
      "differential_evolution step 9: f(x)= -0.259005\n",
      "differential_evolution step 10: f(x)= -0.259005\n",
      "differential_evolution step 11: f(x)= -0.259005\n",
      "differential_evolution step 12: f(x)= -0.260111\n",
      "differential_evolution step 13: f(x)= -0.260111\n",
      "differential_evolution step 14: f(x)= -0.260111\n",
      "differential_evolution step 15: f(x)= -0.260111\n",
      "differential_evolution step 16: f(x)= -0.260111\n",
      "differential_evolution step 17: f(x)= -0.26057\n",
      "differential_evolution step 18: f(x)= -0.26057\n",
      "differential_evolution step 19: f(x)= -0.260877\n",
      "differential_evolution step 20: f(x)= -0.260899\n",
      "variance optimization tolerance of changed to:  0.026089932120288303\n",
      "Next points to be requested: \n",
      "[[4.96433562 8.13252902]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  163\n",
      "Run Time:  27.809262990951538      seconds\n",
      "Number of measurements:  163\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026089932120288303\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02488431692222911\n",
      "Next points to be requested: \n",
      "[[7.24016419 9.30601539]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  164\n",
      "Run Time:  27.82902216911316      seconds\n",
      "Number of measurements:  164\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02488431692222911\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.254135\n",
      "differential_evolution step 2: f(x)= -0.258779\n",
      "differential_evolution step 3: f(x)= -0.258779\n",
      "differential_evolution step 4: f(x)= -0.258779\n",
      "differential_evolution step 5: f(x)= -0.258779\n",
      "differential_evolution step 6: f(x)= -0.259188\n",
      "differential_evolution step 7: f(x)= -0.259188\n",
      "differential_evolution step 8: f(x)= -0.259188\n",
      "differential_evolution step 9: f(x)= -0.259188\n",
      "differential_evolution step 10: f(x)= -0.259188\n",
      "differential_evolution step 11: f(x)= -0.259188\n",
      "differential_evolution step 12: f(x)= -0.259188\n",
      "differential_evolution step 13: f(x)= -0.259188\n",
      "differential_evolution step 14: f(x)= -0.259188\n",
      "differential_evolution step 15: f(x)= -0.260562\n",
      "variance optimization tolerance of changed to:  0.026056193051299123\n",
      "Next points to be requested: \n",
      "[[8.68025224 4.5979175 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  165\n",
      "Run Time:  28.389576196670532      seconds\n",
      "Number of measurements:  165\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026056193051299123\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02369876470143849\n",
      "Next points to be requested: \n",
      "[[6.3578378 0.       ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  166\n",
      "Run Time:  28.419016122817993      seconds\n",
      "Number of measurements:  166\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02369876470143849\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.255072\n",
      "differential_evolution step 2: f(x)= -0.255072\n",
      "differential_evolution step 3: f(x)= -0.255072\n",
      "differential_evolution step 4: f(x)= -0.25593\n",
      "differential_evolution step 5: f(x)= -0.25593\n",
      "differential_evolution step 6: f(x)= -0.258383\n",
      "differential_evolution step 7: f(x)= -0.258383\n",
      "differential_evolution step 8: f(x)= -0.258383\n",
      "differential_evolution step 9: f(x)= -0.258383\n",
      "differential_evolution step 10: f(x)= -0.258383\n",
      "variance optimization tolerance of changed to:  0.02583833782926366\n",
      "Next points to be requested: \n",
      "[[9.21111504 8.11649533]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  167\n",
      "Run Time:  28.861799240112305      seconds\n",
      "Number of measurements:  167\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02583833782926366\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02271971212380281\n",
      "Next points to be requested: \n",
      "[[10.          7.34788252]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  168\n",
      "Run Time:  28.880810737609863      seconds\n",
      "Number of measurements:  168\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02271971212380281\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.253674\n",
      "differential_evolution step 2: f(x)= -0.253674\n",
      "differential_evolution step 3: f(x)= -0.253674\n",
      "differential_evolution step 4: f(x)= -0.255429\n",
      "differential_evolution step 5: f(x)= -0.256284\n",
      "differential_evolution step 6: f(x)= -0.256284\n",
      "differential_evolution step 7: f(x)= -0.256284\n",
      "differential_evolution step 8: f(x)= -0.256284\n",
      "differential_evolution step 9: f(x)= -0.256284\n",
      "differential_evolution step 10: f(x)= -0.256284\n",
      "differential_evolution step 11: f(x)= -0.256284\n",
      "differential_evolution step 12: f(x)= -0.256284\n",
      "variance optimization tolerance of changed to:  0.02562841678792654\n",
      "Next points to be requested: \n",
      "[[9.33081864 6.44723743]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  169\n",
      "Run Time:  29.367974996566772      seconds\n",
      "Number of measurements:  169\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02562841678792654\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02310370077205998\n",
      "Next points to be requested: \n",
      "[[5.45361749 1.77256597]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  170\n",
      "Run Time:  29.38551425933838      seconds\n",
      "Number of measurements:  170\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02310370077205998\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.252701\n",
      "differential_evolution step 2: f(x)= -0.252701\n",
      "differential_evolution step 3: f(x)= -0.252701\n",
      "differential_evolution step 4: f(x)= -0.253792\n",
      "differential_evolution step 5: f(x)= -0.253792\n",
      "differential_evolution step 6: f(x)= -0.253792\n",
      "differential_evolution step 7: f(x)= -0.254186\n",
      "differential_evolution step 8: f(x)= -0.254865\n",
      "differential_evolution step 9: f(x)= -0.254865\n",
      "differential_evolution step 10: f(x)= -0.258995\n",
      "differential_evolution step 11: f(x)= -0.258995\n",
      "differential_evolution step 12: f(x)= -0.258995\n",
      "differential_evolution step 13: f(x)= -0.258995\n",
      "differential_evolution step 14: f(x)= -0.258995\n",
      "differential_evolution step 15: f(x)= -0.2592\n",
      "differential_evolution step 16: f(x)= -0.2592\n",
      "differential_evolution step 17: f(x)= -0.259312\n",
      "variance optimization tolerance of changed to:  0.025931212446361464\n",
      "Next points to be requested: \n",
      "[[6.43922331 6.19351334]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  171\n",
      "Run Time:  30.059934377670288      seconds\n",
      "Number of measurements:  171\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025931212446361464\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021226824973204272\n",
      "Next points to be requested: \n",
      "[[4.77632491 5.0815755 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  172\n",
      "Run Time:  30.11234474182129      seconds\n",
      "Number of measurements:  172\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021226824973204272\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.255607\n",
      "differential_evolution step 2: f(x)= -0.255607\n",
      "differential_evolution step 3: f(x)= -0.255607\n",
      "differential_evolution step 4: f(x)= -0.255607\n",
      "differential_evolution step 5: f(x)= -0.262817\n",
      "differential_evolution step 6: f(x)= -0.262817\n",
      "differential_evolution step 7: f(x)= -0.262817\n",
      "differential_evolution step 8: f(x)= -0.262817\n",
      "differential_evolution step 9: f(x)= -0.264063\n",
      "differential_evolution step 10: f(x)= -0.264063\n",
      "differential_evolution step 11: f(x)= -0.267809\n",
      "differential_evolution step 12: f(x)= -0.267809\n",
      "differential_evolution step 13: f(x)= -0.267809\n",
      "differential_evolution step 14: f(x)= -0.267809\n",
      "differential_evolution step 15: f(x)= -0.267809\n",
      "differential_evolution step 16: f(x)= -0.267809\n",
      "differential_evolution step 17: f(x)= -0.267809\n",
      "differential_evolution step 18: f(x)= -0.267809\n",
      "differential_evolution step 19: f(x)= -0.267809\n",
      "differential_evolution step 20: f(x)= -0.268191\n",
      "variance optimization tolerance of changed to:  0.026819132841111433\n",
      "Next points to be requested: \n",
      "[[9.98991425 4.04200033]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  173\n",
      "Run Time:  30.875476598739624      seconds\n",
      "Number of measurements:  173\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026819132841111433\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02532681849767412\n",
      "Next points to be requested: \n",
      "[[6.78742099 5.21829593]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  174\n",
      "Run Time:  30.89631938934326      seconds\n",
      "Number of measurements:  174\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02532681849767412\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.250235\n",
      "differential_evolution step 2: f(x)= -0.250235\n",
      "differential_evolution step 3: f(x)= -0.255232\n",
      "differential_evolution step 4: f(x)= -0.255232\n",
      "differential_evolution step 5: f(x)= -0.255232\n",
      "differential_evolution step 6: f(x)= -0.255232\n",
      "differential_evolution step 7: f(x)= -0.255232\n",
      "differential_evolution step 8: f(x)= -0.255232\n",
      "differential_evolution step 9: f(x)= -0.255232\n",
      "differential_evolution step 10: f(x)= -0.255232\n",
      "differential_evolution step 11: f(x)= -0.255232\n",
      "differential_evolution step 12: f(x)= -0.255232\n",
      "differential_evolution step 13: f(x)= -0.255232\n",
      "differential_evolution step 14: f(x)= -0.255232\n",
      "differential_evolution step 15: f(x)= -0.255232\n",
      "differential_evolution step 16: f(x)= -0.255232\n",
      "differential_evolution step 17: f(x)= -0.255495\n",
      "differential_evolution step 18: f(x)= -0.256906\n",
      "differential_evolution step 19: f(x)= -0.256906\n",
      "differential_evolution step 20: f(x)= -0.256906\n",
      "variance optimization tolerance of changed to:  0.025690560945961224\n",
      "Next points to be requested: \n",
      "[[9.16419805 9.14859029]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  175\n",
      "Run Time:  31.70237445831299      seconds\n",
      "Number of measurements:  175\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025690560945961224\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021660540606071987\n",
      "Next points to be requested: \n",
      "[[9.00495164 2.4728601 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  176\n",
      "Run Time:  31.72344470024109      seconds\n",
      "Number of measurements:  176\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021660540606071987\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 1: f(x)= -0.247632\n",
      "differential_evolution step 2: f(x)= -0.251396\n",
      "differential_evolution step 3: f(x)= -0.251396\n",
      "differential_evolution step 4: f(x)= -0.261225\n",
      "differential_evolution step 5: f(x)= -0.261225\n",
      "differential_evolution step 6: f(x)= -0.261225\n",
      "differential_evolution step 7: f(x)= -0.261225\n",
      "differential_evolution step 8: f(x)= -0.261225\n",
      "differential_evolution step 9: f(x)= -0.261225\n",
      "differential_evolution step 10: f(x)= -0.261225\n",
      "differential_evolution step 11: f(x)= -0.261225\n",
      "differential_evolution step 12: f(x)= -0.261225\n",
      "differential_evolution step 13: f(x)= -0.261225\n",
      "differential_evolution step 14: f(x)= -0.261225\n",
      "differential_evolution step 15: f(x)= -0.261225\n",
      "differential_evolution step 16: f(x)= -0.261225\n",
      "differential_evolution step 17: f(x)= -0.261225\n",
      "differential_evolution step 18: f(x)= -0.261515\n",
      "differential_evolution step 19: f(x)= -0.261515\n",
      "differential_evolution step 20: f(x)= -0.261515\n",
      "variance optimization tolerance of changed to:  0.026151545533789157\n",
      "Next points to be requested: \n",
      "[[0.03115321 0.78636015]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  177\n",
      "Run Time:  32.48037266731262      seconds\n",
      "Number of measurements:  177\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.026151545533789157\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.024051825421441454\n",
      "Next points to be requested: \n",
      "[[2.07085883 1.71235685]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  178\n",
      "Run Time:  32.502609729766846      seconds\n",
      "Number of measurements:  178\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024051825421441454\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.250968\n",
      "differential_evolution step 2: f(x)= -0.253067\n",
      "differential_evolution step 3: f(x)= -0.253067\n",
      "differential_evolution step 4: f(x)= -0.253067\n",
      "differential_evolution step 5: f(x)= -0.253067\n",
      "differential_evolution step 6: f(x)= -0.253067\n",
      "differential_evolution step 7: f(x)= -0.255664\n",
      "differential_evolution step 8: f(x)= -0.255664\n",
      "differential_evolution step 9: f(x)= -0.256242\n",
      "differential_evolution step 10: f(x)= -0.256242\n",
      "differential_evolution step 11: f(x)= -0.256242\n",
      "differential_evolution step 12: f(x)= -0.256242\n",
      "differential_evolution step 13: f(x)= -0.256242\n",
      "differential_evolution step 14: f(x)= -0.256242\n",
      "differential_evolution step 15: f(x)= -0.256242\n",
      "differential_evolution step 16: f(x)= -0.256269\n",
      "variance optimization tolerance of changed to:  0.025626873338407515\n",
      "Next points to be requested: \n",
      "[[6.04329004 3.05061733]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  179\n",
      "Run Time:  33.11427140235901      seconds\n",
      "Number of measurements:  179\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025626873338407515\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02207147813433851\n",
      "Next points to be requested: \n",
      "[[7.49609347 3.43837349]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  180\n",
      "Run Time:  33.13382840156555      seconds\n",
      "Number of measurements:  180\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02207147813433851\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.246434\n",
      "differential_evolution step 2: f(x)= -0.246434\n",
      "differential_evolution step 3: f(x)= -0.251438\n",
      "differential_evolution step 4: f(x)= -0.251438\n",
      "differential_evolution step 5: f(x)= -0.251438\n",
      "differential_evolution step 6: f(x)= -0.252702\n",
      "differential_evolution step 7: f(x)= -0.252702\n",
      "differential_evolution step 8: f(x)= -0.255284\n",
      "differential_evolution step 9: f(x)= -0.255284\n",
      "differential_evolution step 10: f(x)= -0.255684\n",
      "differential_evolution step 11: f(x)= -0.255684\n",
      "differential_evolution step 12: f(x)= -0.255684\n",
      "differential_evolution step 13: f(x)= -0.255684\n",
      "differential_evolution step 14: f(x)= -0.255936\n",
      "differential_evolution step 15: f(x)= -0.256345\n",
      "differential_evolution step 16: f(x)= -0.256345\n",
      "differential_evolution step 17: f(x)= -0.256345\n",
      "differential_evolution step 18: f(x)= -0.256345\n",
      "differential_evolution step 19: f(x)= -0.25636\n",
      "differential_evolution step 20: f(x)= -0.256602\n",
      "variance optimization tolerance of changed to:  0.02566018139406741\n",
      "Next points to be requested: \n",
      "[[0.68187085 5.59401816]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  181\n",
      "Run Time:  33.98890018463135      seconds\n",
      "Number of measurements:  181\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02566018139406741\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.023507651761742507\n",
      "Next points to be requested: \n",
      "[[2.91696455 7.38296941]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  182\n",
      "Run Time:  34.01321077346802      seconds\n",
      "Number of measurements:  182\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.023507651761742507\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.248869\n",
      "differential_evolution step 2: f(x)= -0.2495\n",
      "differential_evolution step 3: f(x)= -0.2495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 4: f(x)= -0.253971\n",
      "differential_evolution step 5: f(x)= -0.253971\n",
      "differential_evolution step 6: f(x)= -0.253971\n",
      "differential_evolution step 7: f(x)= -0.253971\n",
      "differential_evolution step 8: f(x)= -0.25529\n",
      "differential_evolution step 9: f(x)= -0.25529\n",
      "differential_evolution step 10: f(x)= -0.25529\n",
      "differential_evolution step 11: f(x)= -0.25529\n",
      "differential_evolution step 12: f(x)= -0.255445\n",
      "differential_evolution step 13: f(x)= -0.255445\n",
      "differential_evolution step 14: f(x)= -0.255445\n",
      "differential_evolution step 15: f(x)= -0.255445\n",
      "differential_evolution step 16: f(x)= -0.255445\n",
      "differential_evolution step 17: f(x)= -0.255445\n",
      "differential_evolution step 18: f(x)= -0.255643\n",
      "variance optimization tolerance of changed to:  0.02556430110360172\n",
      "Next points to be requested: \n",
      "[[1.32157126 0.77830349]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  183\n",
      "Run Time:  34.83079719543457      seconds\n",
      "Number of measurements:  183\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02556430110360172\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.022645676543233753\n",
      "Next points to be requested: \n",
      "[[2.28835656 2.98645973]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  184\n",
      "Run Time:  34.85733413696289      seconds\n",
      "Number of measurements:  184\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.022645676543233753\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.24727\n",
      "differential_evolution step 2: f(x)= -0.247631\n",
      "differential_evolution step 3: f(x)= -0.247631\n",
      "differential_evolution step 4: f(x)= -0.247631\n",
      "differential_evolution step 5: f(x)= -0.247631\n",
      "differential_evolution step 6: f(x)= -0.247631\n",
      "differential_evolution step 7: f(x)= -0.254603\n",
      "differential_evolution step 8: f(x)= -0.254603\n",
      "differential_evolution step 9: f(x)= -0.254603\n",
      "differential_evolution step 10: f(x)= -0.255282\n",
      "differential_evolution step 11: f(x)= -0.255282\n",
      "differential_evolution step 12: f(x)= -0.255282\n",
      "differential_evolution step 13: f(x)= -0.255282\n",
      "differential_evolution step 14: f(x)= -0.255362\n",
      "variance optimization tolerance of changed to:  0.02553617630227228\n",
      "Next points to be requested: \n",
      "[[7.43642744 6.58776286]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  185\n",
      "Run Time:  35.45148491859436      seconds\n",
      "Number of measurements:  185\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02553617630227228\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.021563736854274144\n",
      "Next points to be requested: \n",
      "[[6.00703135 0.29277521]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  186\n",
      "Run Time:  35.473161697387695      seconds\n",
      "Number of measurements:  186\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.021563736854274144\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.250175\n",
      "differential_evolution step 2: f(x)= -0.252645\n",
      "differential_evolution step 3: f(x)= -0.252645\n",
      "differential_evolution step 4: f(x)= -0.252645\n",
      "differential_evolution step 5: f(x)= -0.252645\n",
      "differential_evolution step 6: f(x)= -0.252645\n",
      "differential_evolution step 7: f(x)= -0.252645\n",
      "differential_evolution step 8: f(x)= -0.252645\n",
      "differential_evolution step 9: f(x)= -0.252645\n",
      "differential_evolution step 10: f(x)= -0.252645\n",
      "differential_evolution step 11: f(x)= -0.252645\n",
      "differential_evolution step 12: f(x)= -0.252645\n",
      "differential_evolution step 13: f(x)= -0.253044\n",
      "differential_evolution step 14: f(x)= -0.253044\n",
      "differential_evolution step 15: f(x)= -0.253044\n",
      "differential_evolution step 16: f(x)= -0.253044\n",
      "differential_evolution step 17: f(x)= -0.254864\n",
      "differential_evolution step 18: f(x)= -0.254864\n",
      "differential_evolution step 19: f(x)= -0.255224\n",
      "differential_evolution step 20: f(x)= -0.255224\n",
      "variance optimization tolerance of changed to:  0.025522416711720147\n",
      "Next points to be requested: \n",
      "[[5.11199256 4.09094744]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  187\n",
      "Run Time:  36.29532504081726      seconds\n",
      "Number of measurements:  187\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025522416711720147\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance optimization tolerance of changed to:  0.024577004886526067\n",
      "Next points to be requested: \n",
      "[[3.12281834 2.52425328]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  188\n",
      "Run Time:  36.38733124732971      seconds\n",
      "Number of measurements:  188\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.024577004886526067\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.250802\n",
      "differential_evolution step 2: f(x)= -0.250802\n",
      "differential_evolution step 3: f(x)= -0.250802\n",
      "differential_evolution step 4: f(x)= -0.250802\n",
      "differential_evolution step 5: f(x)= -0.255111\n",
      "differential_evolution step 6: f(x)= -0.255111\n",
      "differential_evolution step 7: f(x)= -0.255111\n",
      "differential_evolution step 8: f(x)= -0.255111\n",
      "differential_evolution step 9: f(x)= -0.255111\n",
      "differential_evolution step 10: f(x)= -0.255111\n",
      "differential_evolution step 11: f(x)= -0.255111\n",
      "differential_evolution step 12: f(x)= -0.255111\n",
      "differential_evolution step 13: f(x)= -0.255111\n",
      "differential_evolution step 14: f(x)= -0.256003\n",
      "differential_evolution step 15: f(x)= -0.256003\n",
      "differential_evolution step 16: f(x)= -0.256776\n",
      "differential_evolution step 17: f(x)= -0.257436\n",
      "differential_evolution step 18: f(x)= -0.257436\n",
      "differential_evolution step 19: f(x)= -0.257485\n",
      "variance optimization tolerance of changed to:  0.02574851489443207\n",
      "Next points to be requested: \n",
      "[[3.66527953 0.68517256]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  189\n",
      "Run Time:  37.18244957923889      seconds\n",
      "Number of measurements:  189\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02574851489443207\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02333124567944686\n",
      "Next points to be requested: \n",
      "[[5.70589654 9.74080775]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  190\n",
      "Run Time:  37.20650267601013      seconds\n",
      "Number of measurements:  190\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02333124567944686\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.247506\n",
      "differential_evolution step 2: f(x)= -0.24955\n",
      "differential_evolution step 3: f(x)= -0.24955\n",
      "differential_evolution step 4: f(x)= -0.24955\n",
      "differential_evolution step 5: f(x)= -0.24955\n",
      "differential_evolution step 6: f(x)= -0.24955\n",
      "differential_evolution step 7: f(x)= -0.24955\n",
      "differential_evolution step 8: f(x)= -0.24955\n",
      "differential_evolution step 9: f(x)= -0.24955\n",
      "differential_evolution step 10: f(x)= -0.24955\n",
      "differential_evolution step 11: f(x)= -0.24955\n",
      "differential_evolution step 12: f(x)= -0.24955\n",
      "differential_evolution step 13: f(x)= -0.249566\n",
      "differential_evolution step 14: f(x)= -0.249566\n",
      "differential_evolution step 15: f(x)= -0.249566\n",
      "differential_evolution step 16: f(x)= -0.252175\n",
      "differential_evolution step 17: f(x)= -0.252339\n",
      "differential_evolution step 18: f(x)= -0.252339\n",
      "differential_evolution step 19: f(x)= -0.252339\n",
      "differential_evolution step 20: f(x)= -0.25234\n",
      "variance optimization tolerance of changed to:  0.02523395700240095\n",
      "Next points to be requested: \n",
      "[[5.20996291 6.9963628 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  191\n",
      "Run Time:  38.16165900230408      seconds\n",
      "Number of measurements:  191\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02523395700240095\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02077890006576787\n",
      "Next points to be requested: \n",
      "[[5.71899114 3.27468154]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  192\n",
      "Run Time:  38.18231773376465      seconds\n",
      "Number of measurements:  192\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.02077890006576787\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.246771\n",
      "differential_evolution step 2: f(x)= -0.246771\n",
      "differential_evolution step 3: f(x)= -0.249503\n",
      "differential_evolution step 4: f(x)= -0.249503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 5: f(x)= -0.249503\n",
      "differential_evolution step 6: f(x)= -0.249503\n",
      "differential_evolution step 7: f(x)= -0.249503\n",
      "differential_evolution step 8: f(x)= -0.251935\n",
      "differential_evolution step 9: f(x)= -0.251935\n",
      "differential_evolution step 10: f(x)= -0.251935\n",
      "differential_evolution step 11: f(x)= -0.251935\n",
      "differential_evolution step 12: f(x)= -0.251935\n",
      "differential_evolution step 13: f(x)= -0.251935\n",
      "differential_evolution step 14: f(x)= -0.251935\n",
      "differential_evolution step 15: f(x)= -0.252003\n",
      "differential_evolution step 16: f(x)= -0.25242\n",
      "differential_evolution step 17: f(x)= -0.25242\n",
      "differential_evolution step 18: f(x)= -0.25242\n",
      "differential_evolution step 19: f(x)= -0.25242\n",
      "differential_evolution step 20: f(x)= -0.252448\n",
      "variance optimization tolerance of changed to:  0.02524481403279616\n",
      "Next points to be requested: \n",
      "[[4.59912928 2.64076183]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  193\n",
      "Run Time:  39.138049840927124      seconds\n",
      "Number of measurements:  193\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.02524481403279616\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.019368079752458137\n",
      "Next points to be requested: \n",
      "[[8.54812555 9.77807148]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  194\n",
      "Run Time:  39.16228628158569      seconds\n",
      "Number of measurements:  194\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.019368079752458137\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.245068\n",
      "differential_evolution step 2: f(x)= -0.245838\n",
      "differential_evolution step 3: f(x)= -0.245838\n",
      "differential_evolution step 4: f(x)= -0.245838\n",
      "differential_evolution step 5: f(x)= -0.245838\n",
      "differential_evolution step 6: f(x)= -0.248209\n",
      "differential_evolution step 7: f(x)= -0.248209\n",
      "differential_evolution step 8: f(x)= -0.250352\n",
      "differential_evolution step 9: f(x)= -0.250352\n",
      "differential_evolution step 10: f(x)= -0.250352\n",
      "differential_evolution step 11: f(x)= -0.250635\n",
      "differential_evolution step 12: f(x)= -0.250635\n",
      "differential_evolution step 13: f(x)= -0.250635\n",
      "differential_evolution step 14: f(x)= -0.250635\n",
      "differential_evolution step 15: f(x)= -0.250635\n",
      "differential_evolution step 16: f(x)= -0.250635\n",
      "differential_evolution step 17: f(x)= -0.250635\n",
      "differential_evolution step 18: f(x)= -0.250635\n",
      "differential_evolution step 19: f(x)= -0.250635\n",
      "differential_evolution step 20: f(x)= -0.250635\n",
      "variance optimization tolerance of changed to:  0.025063505359832785\n",
      "Next points to be requested: \n",
      "[[2.78206375 6.06649356]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  195\n",
      "Run Time:  40.079551458358765      seconds\n",
      "Number of measurements:  195\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025063505359832785\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.022588289230299666\n",
      "Next points to be requested: \n",
      "[[8.59668173 0.67985581]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  196\n",
      "Run Time:  40.09535574913025      seconds\n",
      "Number of measurements:  196\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.022588289230299666\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.24229\n",
      "differential_evolution step 2: f(x)= -0.243475\n",
      "differential_evolution step 3: f(x)= -0.247577\n",
      "differential_evolution step 4: f(x)= -0.248095\n",
      "differential_evolution step 5: f(x)= -0.24857\n",
      "differential_evolution step 6: f(x)= -0.251634\n",
      "differential_evolution step 7: f(x)= -0.251634\n",
      "differential_evolution step 8: f(x)= -0.251634\n",
      "differential_evolution step 9: f(x)= -0.251634\n",
      "differential_evolution step 10: f(x)= -0.251634\n",
      "differential_evolution step 11: f(x)= -0.251634\n",
      "differential_evolution step 12: f(x)= -0.251634\n",
      "differential_evolution step 13: f(x)= -0.251634\n",
      "differential_evolution step 14: f(x)= -0.251634\n",
      "differential_evolution step 15: f(x)= -0.251634\n",
      "differential_evolution step 16: f(x)= -0.251634\n",
      "differential_evolution step 17: f(x)= -0.251634\n",
      "differential_evolution step 18: f(x)= -0.251634\n",
      "differential_evolution step 19: f(x)= -0.251634\n",
      "variance optimization tolerance of changed to:  0.025163381644966766\n",
      "Next points to be requested: \n",
      "[[8.44699213 1.57316352]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  197\n",
      "Run Time:  40.92713713645935      seconds\n",
      "Number of measurements:  197\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.025163381644966766\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.020603717113067604\n",
      "Next points to be requested: \n",
      "[[3.08177992 6.40669692]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  198\n",
      "Run Time:  40.96216869354248      seconds\n",
      "Number of measurements:  198\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  global\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  global\n",
      "tolerance:  0.020603717113067604\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "differential_evolution step 1: f(x)= -0.244217\n",
      "differential_evolution step 2: f(x)= -0.244217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "differential_evolution step 3: f(x)= -0.246461\n",
      "differential_evolution step 4: f(x)= -0.248481\n",
      "differential_evolution step 5: f(x)= -0.248481\n",
      "differential_evolution step 6: f(x)= -0.248481\n",
      "differential_evolution step 7: f(x)= -0.248481\n",
      "differential_evolution step 8: f(x)= -0.248481\n",
      "differential_evolution step 9: f(x)= -0.248481\n",
      "differential_evolution step 10: f(x)= -0.248585\n",
      "differential_evolution step 11: f(x)= -0.248585\n",
      "differential_evolution step 12: f(x)= -0.248585\n",
      "differential_evolution step 13: f(x)= -0.248585\n",
      "differential_evolution step 14: f(x)= -0.248585\n",
      "differential_evolution step 15: f(x)= -0.249462\n",
      "variance optimization tolerance of changed to:  0.024946231642361014\n",
      "Next points to be requested: \n",
      "[[7.19072949 0.7380853 ]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "Async Hyper-parameter update not successful. I am keeping the old ones.\n",
      "That probbaly means you are not optimizing them asynchronously\n",
      "hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "GPOptimizer updated the Hyperperameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "The Autonomus Experimenter updated the Hyperparameters\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "No training in this round but I tried to update the hyperparameters\n",
      "\n",
      "\n",
      "\n",
      "====================\n",
      "====================\n",
      "iteration:  199\n",
      "Run Time:  41.72059416770935      seconds\n",
      "Number of measurements:  199\n",
      "====================\n",
      "hps:  [0.78280255 7.23059506 7.0944846 ]\n",
      "aks() initiated with hyperparameters: [0.78280255 7.23059506 7.0944846 ]\n",
      "optimization method:  local\n",
      "bounds:  None\n",
      "====================================\n",
      "finding acquisition function maxima...\n",
      "optimization method  local\n",
      "tolerance:  0.024946231642361014\n",
      "population size:  20\n",
      "maximum number of iterations:  20\n",
      "bounds: \n",
      "[[ 0 10]\n",
      " [ 0 10]]\n",
      "cost function parameters:  {}\n",
      "====================================\n",
      "variance optimization tolerance of changed to:  0.02163385455191285\n",
      "Next points to be requested: \n",
      "[[7.63271327 0.57909985]]\n",
      "CAUTION: you have not provided data variances, they will set to be 1 percent of the data values!\n",
      "Training ...\n",
      "async trianing is being killed\n",
      "Cancelling asynchronous training...\n",
      "No asynchronous training to be cancelled, no training is running.\n",
      "Fresh optimization from scratch via global optimization\n",
      "GP training started with  400  data points\n",
      "Hyper-parameter tuning in progress. Old hyperparameters:  [0.78280255 7.23059506 7.0944846 ]  with old log likelihood:  -274.73724762747105\n",
      "method:  local\n",
      "Performing a local update of the hyper parameters.\n",
      "starting hyperparameters:  [0.78280255 7.23059506 7.0944846 ]\n",
      "Attempting a BFGS optimization.\n",
      "maximum number of iterations:  20\n",
      "termination tolerance:  1e-06\n",
      "bounds:  [[1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]\n",
      " [1.e-03 1.e+02]]\n",
      "Local optimization successfully concluded with result:  -289.5117724645899\n",
      "====================================================\n",
      "The autonomous experiment was concluded successfully\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/env python\n",
    "import numpy as np\n",
    "from gpcam.autonomous_experimenter import AutonomousExperimenterfvGP\n",
    "\n",
    "def instrument(data):\n",
    "    for entry in data:\n",
    "        entry[\"values\"] = np.array([np.sin(np.linalg.norm(entry[\"position\"])),\n",
    "                                    np.sin(np.linalg.norm(entry[\"position\"]))])\n",
    "        entry[\"value positions\"] = np.array([[0],[1]])\n",
    "    return data\n",
    "\n",
    "my_fvae = AutonomousExperimenterfvGP(np.array([[0,10],[0,10]]),2,1,\n",
    "                                     instrument,np.ones((3)),np.array([[0.001,100],[0.001,100],[0.001,100]]),\n",
    "                                     init_dataset_size= 10)\n",
    "my_fvae.train()\n",
    "my_fvae.go(N = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-proportion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
